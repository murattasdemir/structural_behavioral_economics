{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augenblick and Rabin, 2019, \"An Experiment on Time Preference and Misprediction in Unpleasant Tasks\", Table 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Authors:  \n",
    "\n",
    "- Massimiliano Pozzi (Bocconi University, pozzi.massimiliano@studbocconi.it)\n",
    "- Salvatore Nunnari (Bocconi University, salvatore.nunnari@unibocconi.it)\n",
    "\n",
    "#### Description:\n",
    "\n",
    "The code in this Jupyter notebook performs the aggregate estimates to replicate column 1 of Table 1\n",
    "\n",
    "This notebook was tested with the following packages versions:\n",
    "- Pozzi:   (Anaconda 4.10.3 on Windows 10 Pro) : python 3.8.3, numpy 1.18.5, pandas 1.0.5, scipy 1.5.0, autograd 1.3\n",
    "- Nunnari: (Anaconda 4.10.1 on macOS 10.15.7): python 3.8.10, numpy 1.20.2, pandas 1.2.4, scipy 1.6.2, autograd 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "\n",
    "from autograd.scipy.stats import norm    \n",
    "import autograd.numpy as np\n",
    "import pandas as pd\n",
    "import scipy.optimize as opt\n",
    "from autograd import grad, hessian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Cleaning and Data Preparation\n",
    "\n",
    "We import the dataset containing the choices of all 100 individuals who participated to the experiment. To guarantee consistency with the authors' results, we then construct the primary sample used for the aggregate estimates. This sample consists of 72 individuals whose individual parameter estimates converged in less than 200 iterations when using the authors' Stata algorithm. In particular, we run the \"03MergeIndMLEAndConstructMainSample.do\" file provided by the authors. This script creates a file named \"ind_to_keep.csv\" which contains the identifiers of the individuals to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the two datasets and drop subjects whose individual estimates do not converge\n",
    "\n",
    "dt = pd.read_stata('../input/decisions_data.dta')    # full sample\n",
    "ind_keep = pd.read_csv('../input/ind_to_keep.csv')   # import csv with ID of subjects to keep \n",
    "\n",
    "# drop subjects whose IDs are not listed in the ind_keep dataframe (28 individuals)\n",
    "\n",
    "dt = dt[dt.wid.isin(ind_keep.wid_col1)] # this is the primary sample for the aggregate estimates (72 individuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove observations when a bonus was offered and create the following dummy variables that will be useful for estimation: pb is equal to one if the subject completed 10 mandatory tasks on subject-day (this is used to estimate the projection bias parameter &alpha;); ind_effort10 and ind_effort110 are equal to one if, respectively, the subject completed 10 or 110 tasks (and they are used for the Tobit correction when computing the likelihood)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove observations when a bonus was offered and create dummy variables. \n",
    "\n",
    "dt = dt[dt.bonusoffered !=1]   # remove observations when a bonus was offered\n",
    "dt['pb']= dt['workdone1']/10   # pb dummy variable. workdone1 can either be 10 or 0, so dividing the variable by 10 creates our dummy\n",
    "dt['ind_effort10']  = (dt['effort']==10).astype(int)   # ind_effort10 dummy\n",
    "dt['ind_effort110'] = (dt['effort']==110).astype(int)  # ind_effort110 dummy\n",
    "dt.index = np.arange(len(dt.wid))                      # correct the index. The index should go from 0 to 8048"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define the Model and the Likelihood (Section 3 in Paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent needs to choose the optimal effort e to solve a simple tradeoff problem between disutility of effort and consumption utility derived from the consequent payment. More specifically, the agent takes a decision at a time k to complete a certain number of tasks at time t and to get paid a wage w per task at time T. Assuming the agent discounts utility using quasi-hyperbolic discounting and has a convex cost function C(e) the problem can be conveniently written as:\n",
    "\n",
    "$$ \\max_{{e}} \\; \\delta^{T-k}⋅(e⋅w)- \\frac{1}{\\beta^{I(k=t)}}⋅\\frac{1}{\\beta_h^{I(p=1)}}⋅\\delta^{t-k}⋅ \\frac{e^\\gamma}{\\phi⋅\\gamma} $$\n",
    "\n",
    "Where the last term is a two parameter power cost function, I(k=t) is an indicator function equal to one if the decision occurs in the same period as the effort and I(p=1) is an indicator that the decision is a prediction, &beta;<sub>h</sub> is the perceived present bias parameter (that is, the agent's degree of awareness of his present bias), and &delta; is the standard time discounting parameter. Taking the derivative of the maximization problem above with respect to effort yields the following first order condition:\n",
    "\n",
    "$$  e^*= \\left(\\frac{\\delta^{T-k}⋅\\phi⋅w}{\\frac{1}{\\beta^{I(k=t)}}⋅\\frac{1}{\\beta_h^{I(p=1)}}⋅\\delta^{t-k}} \\right)^{\\frac{1}{\\gamma-1}} $$\n",
    "\n",
    "This is the optimal effort level, or what we will call in the code the predicted choice. To model heterogeneity, the authors assume that the observed effort is distributed as the predicted effort plus an implementation error which is Gaussian with mean zero and standard deviation sigma, so that the likelihood of observing an effort decision e<sub>j</sub> in the data is equal to:\n",
    "\n",
    "$$ L(e_j)= \\phi \\left(\\frac{e^*_j-e_j}{\\sigma}\\right)$$\n",
    "\n",
    "where &phi; is the pdf of a standard normal. \n",
    "\n",
    "To deal with corner solutions we apply a Tobit correction, so that the likelihood to maximize is:\n",
    "\n",
    "$$ L^{tobit}(e_j)=(1-I(e=10)-I(e=110))⋅\\phi \\left(\\frac{e^*_j-e_j}{\\sigma}\\right) + I(e=10)⋅\\left(1- \\Phi \\left(\\frac{e_j^*-10}{\\sigma}\\right)\\right)+I(e=110)⋅ \\Phi \\left(\\frac{e_j^*-110}{\\sigma}\\right) $$\n",
    "\n",
    "where &Phi;(⋅) is the cdf of a standard normal, while I(e=10) and I(e=110) are the indicators ind_effort10 and ind_effort110 explained above. Note that, to keep the code simple, in this notebook we call effort the number of tasks performed by the agent, that is, the number of tasks chosen by the agent (ranging between 0 and 100) plus the compulsory 10 tasks. In the paper, the authors call effort just the number of tasks chosen by the agent (and, thus, they add 10 tasks to get total effort). This explains the differences between the equations in this notebook and equation (7), (8) and (10) in Section 3 of the paper. \n",
    "\n",
    "Our goal is to minimize the negative of the sum of the logarithms of L<sup>tobit</sup>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function negloglike computes the negative of the log likelihood of observing our data given the parameters of the model.\n",
    "\n",
    "# parameters:\n",
    "\n",
    "# beta is the present bias parameter\n",
    "# betahat is the perceived present bias parameter\n",
    "# delta is the usual time-discounting parameter\n",
    "# gamma and phi are the two parameters controlling the cost of effort function\n",
    "# alpha is the projection bias parameter\n",
    "# sigma is the standard deviation of the normal error term ϵ\n",
    "\n",
    "# args:\n",
    "\n",
    "# netdistance is (T-k)-(t-k) = T-t, the difference between the payment date T and the work time t\n",
    "# wage is the amount paid per task in a certain session\n",
    "# today is a dummy variable equal to one if the decision involves the choice of work today\n",
    "# prediction is a dummy variable equal to one if the decision involves the choice of work in the future\n",
    "# pb is a dummy equal to one if the subject completed 10 mandatory tasks on subject-day \n",
    "# effort is the number of tasks completed by a subject in a session. It can range from a minimum of 10 to a maximum of 110\n",
    "# ind_effort10 is a dummy equal to one if the subject's effort was equal to 10\n",
    "# ind_effort110 is a dummy equal to one if the subject's effort was equal to 110\n",
    "\n",
    "\n",
    "def negloglike(params, *args):\n",
    "    \n",
    "    beta, betahat, delta, gamma, phi, alpha, sigma = params\n",
    "    netdistance, wage, today, prediction, pb, effort, ind_effort10, ind_effort110 = args\n",
    "    \n",
    "    # We use np.array to allow for element-wise operations\n",
    "    \n",
    "    netdistance = np.array(netdistance)\n",
    "    wage = np.array(wage)\n",
    "    today = np.array(today)\n",
    "    prediction = np.array(prediction)\n",
    "    pb = np.array(pb)\n",
    "    effort = np.array(effort)\n",
    "    ind_effort10 = np.array(ind_effort10)\n",
    "    ind_effort110 = np.array(ind_effort110)\n",
    "    \n",
    "    # predchoice is the predicted choice coming from the optimality condition of the subject\n",
    "    \n",
    "    predchoice=((phi*(delta**netdistance)*(beta**today)*(betahat**prediction)*wage)**(1/(gamma-1)))-pb*alpha\n",
    "    \n",
    "    # prob is a 1x8049 vector containing the probability of observing the effort of an individual. If effort is 10 or 110 we apply a Tobit correction\n",
    "    \n",
    "    prob = (1-ind_effort10-ind_effort110)*norm.pdf(effort, predchoice, sigma)+ind_effort10*(1 - norm.cdf((predchoice-effort)/sigma))+ind_effort110*norm.cdf((predchoice-effort)/sigma)\n",
    "            \n",
    "    # we now look at the vector prob and add a small value close to zero if prob=0 or subtract a small value close to zero if prob=1. This is necessary to avoid problems when taking logs\n",
    "        \n",
    "    index_p0 = [i for i in range(0,len(prob)) if prob[i]==0] # vector containing the indexes when prob=0\n",
    "    index_p1 = [i for i in range(0,len(prob)) if prob[i]==1] # vector containing the indexes when prob=1\n",
    "    \n",
    "    # use a for loop to change the values\n",
    "    \n",
    "    for i in index_p0:\n",
    "        prob[i] = 1E-4\n",
    "        \n",
    "    for i in index_p1:\n",
    "        prob[i] = 1 - 1E-4\n",
    "    \n",
    "    negll = - np.sum(np.log(prob)) # negative log likelihood\n",
    "    \n",
    "    return negll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Estimation\n",
    "\n",
    "### Point Estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now estimate the model. First, we need to initialize a vector with the starting parameters for the minimization algorithm. We then minimize the negative log-likelihood function using the scipy.optimize package and the Nelder-Mead algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the initial guesses (same as the ones used by the authors in their do.file) and the arguments for the function to minimize\n",
    "\n",
    "# starting parameters for the algorithm\n",
    "\n",
    "beta_init, betahat_init, delta_init, gamma_init, phi_init, alpha_init, sigma_init = 0.8, 1, 1, 2, 500, 7, 40\n",
    "par_init = [beta_init, betahat_init, delta_init, gamma_init, phi_init, alpha_init, sigma_init]\n",
    "\n",
    "# args necessary for the function to minimize\n",
    "\n",
    "mle_args = (dt['netdistance'],dt['wage'],dt['today'],dt['prediction'],dt['pb'],dt['effort'],dt['ind_effort10'],dt['ind_effort110'])\n",
    "\n",
    "# we now find the estimates using the scipy.optimize package\n",
    "\n",
    "sol = opt.minimize(negloglike, par_init, args=(mle_args), method='Nelder-Mead', options={'maxiter': 1500})\n",
    "res = sol.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Errors\n",
    "\n",
    "We now estimate individual cluster robust standard errors. \n",
    "\n",
    "These are computed by taking the square root of the diagonal elements of the following matrix: \n",
    "\n",
    "$$ Adj⋅(H^{-1} @ G @ H^{-1}) $$ \n",
    "\n",
    "Where Adj is an adjustment for the degree of freedoms and the number of clusters:\n",
    "\n",
    "$$ Adj = \\frac{Nr.observations-1}{Nr.observations-Nr.parameters}⋅\\frac{Nr.clusters}{Nr.clusters-1} $$ \n",
    "\n",
    "H<sup>-1</sup> is the inverse of the hessian of the negative log-likelihood evaluated in the minimum (our estimates), @ stands for matrix multiplication, and G is a 5x5 matrix of gradient contributions. \n",
    "\n",
    "We denote the gradient of the log likelihood function for a generic individual i as follows:\n",
    "\n",
    "$$  g_i(y|\\theta) = [log f_i(y|\\theta)]' = \\frac{\\partial}{\\partial \\theta} log f_i(y|\\theta) $$\n",
    "\n",
    "where &theta; is the parameters vector and f<sub>i</sub>(y|&theta;) the likelihood function. Then G is defined as follows:\n",
    "\n",
    "$$ G = \\sum_j \\left[\\sum_{i \\in c_j}g_i(y|\\hat{\\theta})\\right]^T\\left[\\sum_{i \\in c_j}g_i(y|\\hat{\\theta})\\right] $$\n",
    "\n",
    "where J is the number of clusters (in our case the number of unique individuals = 72) and c<sub>j</sub> is a generic cluster j, that includes all observations for a specific individual (in our case 130). For more information on how to compute standard errors when using maximum likelihood, we refer the reader to David A. Freedman, 2006, [\"On The So-Called 'Huber Sandwich Estimator' and 'Robust Standard Errors'\"](https://snunnari.github.io/freedman.pdf), *The American Statistician*, 60:4, 299-302)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function that computes the matrix of individual gradient contribution G. \n",
    "# We use the autograd package that performs automatic differentiation. Automatic differentiation yields more precise results than finite differences\n",
    "\n",
    "def gradcontr(dt, parameters):\n",
    "    \n",
    "    G = np.zeros((len(parameters), len(parameters))) # A 7x7 matrix \n",
    "    vsingle_grad = []  # This will be a 1x8049 vector whose elements are 1x7 vectors. Each 1x7 vector is the gradient of negloglike for a single observation\n",
    "    \n",
    "    for j in range(0, len(dt.wid)):  # loop over all 8049 observations\n",
    "        \n",
    "        # args needed to compute the individual observation likelihood\n",
    "        args_ind = ([dt['netdistance'][j]],[dt['wage'][j]],[dt['today'][j]],[dt['prediction'][j]],[dt['pb'][j]],\n",
    "                     [dt['effort'][j]],[dt['ind_effort10'][j]],[dt['ind_effort110'][j]]) \n",
    "    \n",
    "        single_grad = np.array(gradfun(parameters, *args_ind))  # 1x7 vector. gradient of the negative log likelihood using only one observation in the dataset\n",
    "        vsingle_grad.append(single_grad)\n",
    "\n",
    "    # we create a two columns dataframe. The first one is the wid, the second one is vsingle_grad. This will simplify summing the gradients\n",
    "    # over a specific individual. Each element in the column gradient is a 1x7 vector.\n",
    "    \n",
    "    dg = pd.DataFrame({'wid': dt.wid, 'gradient': vsingle_grad})\n",
    "    \n",
    "    for wid in np.unique(dt.wid): # loop over the individuals IDs\n",
    "        \n",
    "        ind_grad = [sum(i) for i in zip(*dg.loc[dg['wid'] == wid].gradient)] # we are summing the single observation gradients element-wise.\n",
    "        G += np.outer(ind_grad,ind_grad)                                     # we take the outer product and sum them\n",
    "        \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the individual cluster robust standard errors\n",
    "\n",
    "# Compute the hessian\n",
    "Hfun = hessian(negloglike)\n",
    "hessian = Hfun(res, *mle_args)      # hessian\n",
    "hess_inv = np.linalg.inv(hessian)   # inverse of the hessian\n",
    "\n",
    "# Compute the matrix of gradient contribution\n",
    "gradfun = grad(negloglike)\n",
    "grad_contribution = gradcontr(dt, res)\n",
    "\n",
    "# Compute the adjustment for degree of freedoms and number of clusters\n",
    "adj = (len(dt.wid)-1)/(len(dt.wid)-len(res)) * len(np.unique(dt.wid))/(len(np.unique(dt.wid))-1)\n",
    "\n",
    "varcov_estimates = adj *(hess_inv @ grad_contribution @ hess_inv) # var-cov matrix of our estimates\n",
    "se_cluster = np.sqrt(np.diag((varcov_estimates)))                 # individual cluster robust standard errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis Testing\n",
    "\n",
    "We now do some hypothesis testing on the parameters we obtained. We compute the z-test statistics and the corresponding p-values to check if beta, betahat or delta are statistically different from one. We then compute the p-value of a z-test to check if the parameter for projection bias is statistically different from zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the z-test statistics and the corresponding p-values to check if beta, betahat, delta are statistically different from one\n",
    "\n",
    "zvalues_1 = (np.array(res[0:3])-1)/np.array(se_cluster[0:3]) # the first three elements are for beta (position 0), betahat (position 1) and delta (position 2)\n",
    "pvalues_1 = 2*(1-norm.cdf(np.abs(zvalues_1),0,1))\n",
    "\n",
    "# Now compute the z-test statistics and the corresponding p-value for H0: alpha different from 0\n",
    "\n",
    "zvalue_a = (np.array(res[5]))/np.array(se_cluster[5]) \n",
    "pvalue_a = 2*(1-norm.cdf(np.abs(zvalue_a),0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Print and Save Estimation Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a table with point estimates and individual cluster robust standard errors. We then save the results as a csv file in the output folder and print the results. This replicates Column 1 of Table 1 in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame with the results and save it as a csv file in output. We round the results up to the 3rd decimal.\n",
    "\n",
    "parameters_name = [\"Present Bias β\",\n",
    "                   \"Naive Pres. Bias β_h\",\n",
    "                   \"Discount Factor δ\",\n",
    "                   \"Cost Curvature γ\",\n",
    "                   \"Cost Slope ϕ\",\n",
    "                   \"Proj Task Reduction α\",\n",
    "                   \"Sd of error term σ\"]\n",
    "\n",
    "Table_1 = pd.DataFrame({'parameters':parameters_name,'estimates':np.round(res,3),'standarderr':np.round(se_cluster,3)})\n",
    "\n",
    "Table_1.to_csv('../output/table1_python.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 1: Primary aggregate structural estimation\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parameters</th>\n",
       "      <th>estimates</th>\n",
       "      <th>standarderr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Present Bias β</td>\n",
       "      <td>0.835</td>\n",
       "      <td>0.038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Naive Pres. Bias β_h</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Discount Factor δ</td>\n",
       "      <td>1.003</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cost Curvature γ</td>\n",
       "      <td>2.145</td>\n",
       "      <td>0.070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cost Slope ϕ</td>\n",
       "      <td>723.974</td>\n",
       "      <td>251.855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Proj Task Reduction α</td>\n",
       "      <td>7.307</td>\n",
       "      <td>2.598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sd of error term σ</td>\n",
       "      <td>42.625</td>\n",
       "      <td>3.306</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              parameters  estimates  standarderr\n",
       "0         Present Bias β      0.835        0.038\n",
       "1   Naive Pres. Bias β_h      0.999        0.011\n",
       "2      Discount Factor δ      1.003        0.003\n",
       "3       Cost Curvature γ      2.145        0.070\n",
       "4           Cost Slope ϕ    723.974      251.855\n",
       "5  Proj Task Reduction α      7.307        2.598\n",
       "6     Sd of error term σ     42.625        3.306"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations: 8,049\n",
      "Number of participants: 72\n",
      "Log Likelihood: -28,412\n",
      "H_0(β=1) 0.00\n",
      "H_0(β_h=1): 0.92\n",
      "H_0(α=0): 0.005\n",
      "H_0(δ=1): 0.37\n"
     ]
    }
   ],
   "source": [
    "# Print the results\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "print(\"Table 1: Primary aggregate structural estimation\")\n",
    "display(Table_1)\n",
    "print(\"Number of observations:\",\"{:,}\".format(len(dt.wid)))\n",
    "print(\"Number of participants:\",\"{:,}\".format(len(np.unique(dt.wid))))\n",
    "print(\"Log Likelihood:\",\"{:,.0f}\".format(-sol.fun))\n",
    "print(\"H_0(β=1)\",\"{:,.2f}\" .format(np.round(pvalues_1[0],3)))\n",
    "print(\"H_0(β_h=1):\",\"{:,.2f}\".format(np.round(pvalues_1[1],2)))\n",
    "print(\"H_0(α=0):\",\"{:,.3f}\".format(np.round(pvalue_a,3)))\n",
    "print(\"H_0(δ=1):\",\"{:,.2f}\".format(np.round(pvalues_1[2],2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
