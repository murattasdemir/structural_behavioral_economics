{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23f785fc",
   "metadata": {},
   "source": [
    "# Replication of DellaVigna, List, Malmendier and Rao, 2017, \"Voting To Tell Others\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e68d93",
   "metadata": {},
   "source": [
    "#### Authors:  \n",
    "\n",
    "- Massimiliano Pozzi (Bocconi University, pozzi.massimiliano@studbocconi.it)\n",
    "- Salvatore Nunnari (Bocconi University, salvatore.nunnari@unibocconi.it)\n",
    "\n",
    "The code in this Jupyter notebook performs the benchmark estimates with heterogeneous auxiliary parameters (Table 3, column 1) \n",
    "\n",
    "This notebook was tested with the following packages versions:\n",
    "- Pozzi:   (Anaconda 4.10.3 on Windows 10 Pro) : python 3.8.3, numpy 1.18.5, pandas 1.0.5, scipy 1.5.0, numdifftools 0.9.40\n",
    "- Nunnari: (Anaconda 4.10.1 on macOS 10.15.7): python 3.8.10, numpy 1.20.2, pandas 1.2.4, scipy 1.6.2, numdifftools 0.9.39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f540e992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "import scipy.optimize as opt\n",
    "from scipy.io import loadmat\n",
    "from scipy.linalg import block_diag\n",
    "import numdifftools as ndt\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e114847c",
   "metadata": {},
   "source": [
    "## 1. Data Cleaning and Data Preparation\n",
    "\n",
    "We import the relevant 100 empirical moments and variance-covariance matrix. In addition to those moments we manually add the baseline turnout and its standard error. For the benchmark specification we thus need 101 moments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bcce5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the empirical moments and the var-cov matrix\n",
    "\n",
    "dt_empmoments = loadmat('../input/Moments.mat')                           # load the matlab file containing the moments\n",
    "emp_moments = [dt_empmoments[\"Moments\"]]\n",
    "emp_moments = [item for sublist in emp_moments for item in sublist]       # flatten list of empirical moments \n",
    "emp_moments.append(np.array([0.6000]))                                    # add baseline turnout moment\n",
    "emp_moments = np.ndarray.flatten(np.array(emp_moments))                   # flatten the array. This array contains the 101 moments we will use\n",
    "\n",
    "emp_moments_varcov =  dt_empmoments[\"VCcontrol\"]                          # 100x100 var-cov matrix of empirical moments\n",
    "emp_moments_varcov = block_diag(emp_moments_varcov,np.diag([0.0109**2]))  # Add variance for the last moment\n",
    "W = np.linalg.inv(np.diag(np.diag(emp_moments_varcov)))                   # diagonal of the inverse of the var-cov matrix. This will be the weighting matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d479db57",
   "metadata": {},
   "source": [
    "## 2. Define the Model and the estimation strategy (Sections 2 and 5 in the Paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a983fe",
   "metadata": {},
   "source": [
    "The model is a standard model of voting augmented by considering also the expected value of social image. The authors want to estimate in fact a model of voting \"because others will ask\", looking for example at the pride of voting or the disutility of lying when asked if you voted or not. Individuals vote if they get a positive net utility from voting:\n",
    "\n",
    "$$ pV + g - c + N[max(s_V, s_N-L)-max(s_N, s_V-L)] \\geq 0 $$\n",
    "\n",
    "where the first term is the expected utility of being a pivotal voter, g is the warm glow from voting and -c the transaction costs associated to voting (for example going to the polls). These first three terms are standard in a model of voting and their sum will be simply called &epsilon; in the code. What is interesting is the last term, which represents the social image of voting. An individual expects to be asked N times whether she voted or not and can be truthful or lie. s<sub>V</sub> is the utility she gains by telling others she voted, s<sub>N</sub> is the utility from appearing to be a non-voter and L represents the cost of lying. These three are the main parameters of interest of the study, but given the door-to-door survey structure of the field experiment and the numerous different treatments, the authors need to consider and estimate a bigger set of parameters, for example the baseline probability of being at home, the cost of altering this probability if you saw the flyer informing you about the survey, the utility a person gets from completing a survey etc. All these parameters will be explained in more details in the code below. \n",
    "\n",
    "The estimation method is simulated minimum distance, where we impose simulated moments being equal to the empirical moments observed in the data. These simulated moments come from the fact that s<sub>V</sub>, s<sub>N</sub> and &epsilon; are assumed to be stochastic and heterogeneous. This is because they influence not only the decision wheter to answer the survey or not or to be truthful or lie, but also the turnout decision. The model estimation acknowledge this dual role by first drawing values for s<sub>V</sub>, s<sub>N</sub> and &epsilon; from their distribution (determining the voting status) and then looking at the behavior of the simulated individual in the different treatments. What we will be minimizing in pratice is the weighted sum of squared distances between simulated and empirical moments:\n",
    "\n",
    "$$ \\min_\\xi \\; \\; (m_N(\\xi)-\\hat{m})'W(m_N(\\xi)-\\hat{m}) $$\n",
    "\n",
    "where the first term in parenthesis is the vector containing the simulated moments, with &xi; being the set of parameters, and the second term is the vector containing the simulated moments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a039c773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function computes the relevant 101 moments for the benchmark estimates by simulating N individuals. \n",
    "# rand_set is a vector of vectors containing the random draws for s_V, s_N, Ïµ, s\n",
    "\n",
    "\n",
    "def voteSimEndogenousVoting_vary(parameters, rand_set):\n",
    "\n",
    "# Parameters: \n",
    "# h0 = baseline probability of being at home\n",
    "# r  = probability of seeing the flyer\n",
    "# eta = elasticity of response to sorting in and out of the house after seeing the flyer\n",
    "# s = how much you like doing a survey\n",
    "# S = social pressure cost of doing a survey\n",
    "# sv = value of saying you voted (appearing as a voter)\n",
    "# sn = value of saying you didn't vote (appearing as a non-voter)\n",
    "# rho = correlation between sv and sn (set to 0 in the benchmark estimate)\n",
    "# eps = all other reasons to vote (expected value of being pivotal + warm glow - transaction costs)\n",
    "\n",
    "# Parameters that vary between voters and non-voters. sub v for voters and sub nv for non-voters\n",
    "\n",
    "    N = 5.4                     # Number of times asked if you voted or not in Congressional\n",
    "    N_P = 10.1                  # Times asked in presidential\n",
    "    h0_v    = parameters[0]\n",
    "    h0_nv   = parameters[1]\n",
    "    r_v     = parameters[2]\n",
    "    r_nv    = parameters[3]\n",
    "    eta_v   = parameters[4]\n",
    "    eta_nv  = parameters[5]\n",
    "    mu_s_v  = parameters[6]\n",
    "    mu_s_nv = parameters[7]\n",
    "    sigma_s_v  = parameters[8]\n",
    "    sigma_s_nv = parameters[9]\n",
    "    S_svy_v    = parameters[10]\n",
    "    S_svy_nv   = parameters[11]\n",
    "    timeval_v  = parameters[12]\n",
    "    timeval_nv = parameters[13]\n",
    "\n",
    "# Parameters that don't vary between voters and non-voters\n",
    "\n",
    "    mu_sv = parameters[14]\n",
    "    mu_sn = parameters[15]\n",
    "    sigma_svn = parameters[16] # there should be sigma_v and sigma_nv but sigma_sv = sigma_sn in benchmark estimate\n",
    "    \n",
    "    L = parameters[17]\n",
    "    mu_eps = parameters[18]\n",
    "    sigma_eps = parameters[19] \n",
    "    rho = 0                    # Set to zero in benchmark\n",
    "    \n",
    "# Draw from random variables (distributions of s_v, s_n, eps) \n",
    "\n",
    "    rand_set0 = rand_set[0]\n",
    "    rand_set1 = rand_set[1]\n",
    "    rand_set2 = rand_set[2]\n",
    "    rand_set3 = rand_set[3]\n",
    "    \n",
    "    eps = mu_eps + sigma_eps * rand_set1\n",
    "    sv  = mu_sv  + sigma_svn * rand_set2\n",
    "    sn  = mu_sn  + sigma_svn * rand_set3\n",
    "\n",
    "# Look if random person votes or not\n",
    "\n",
    "    sigVal = (np.maximum(sv,sn-L) - np.maximum(sn,sv-L))    # social image value of voting\n",
    "    sigVal_x_N = sigVal*N                                   # asked N times\n",
    "    utilityVoting = sigVal_x_N + eps                        # net utility of voting\n",
    "    voted = utilityVoting > 0                               # true if the simulated individual votes or false if not\n",
    "    voted = voted.astype(int)\n",
    "\n",
    "# Get indices for those who votes and those who do not\n",
    "\n",
    "    voterIndex    = [vot for vot in range(len(voted)) if voted[vot] == 1]\n",
    "    nonvoterIndex = [vot for vot in range(len(voted)) if voted[vot] != 1]\n",
    "\n",
    "# Share who turn out in the control group (no GOTV intervention)\n",
    "\n",
    "    Turnout_control = np.mean(voted)  \n",
    "\n",
    "# Make vectors with voter and non-voter parameters based on whether voted in the control experiment\n",
    "\n",
    "    h0 = voted*h0_v + (1-voted)*h0_nv\n",
    "    r  = voted*r_v  + (1-voted)*r_nv\n",
    "    eta = voted*eta_v + (1-voted)*eta_nv\n",
    "    S_svy = voted*S_svy_v + (1-voted)*S_svy_nv\n",
    "    timeval = voted*timeval_v + (1-voted)*timeval_nv\n",
    "\n",
    "# Net utility of voting if seen the GOTV flyer\n",
    "\n",
    "    sigVal_x_N_GOTV = (N+h0)*sigVal\n",
    "    utilityVoting_GOTV = sigVal_x_N_GOTV + eps \n",
    "    voted_GOTV = utilityVoting_GOTV>0\n",
    "\n",
    "# Share who turn out with the GOTV intervention assume everyone sees the flyer, counts as N+h0 times asked\n",
    "\n",
    "    Turnout_GOTV = np.mean(voted_GOTV)\n",
    "\n",
    "# Presidential\n",
    "\n",
    "    sigVal_x_N_P = N_P*sigVal                   \n",
    "    sigVal_x_N_P_GOTV = (N_P+h0)*sigVal \n",
    "    utilityVoting_P = sigVal_x_N_P + eps\n",
    "    utilityVoting_P_GOTV = sigVal_x_N_P_GOTV + eps\n",
    "    voted_P = utilityVoting_P>0\n",
    "    voted_P_GOTV = utilityVoting_P_GOTV>0 \n",
    "\n",
    "# Turnout \n",
    "\n",
    "    Turnout_P_control = np.mean(voted_P)\n",
    "    Turnout_P_GOTV = np.mean(voted_P_GOTV)\n",
    "\n",
    "# Draw random s, Utilities of doing the survey for voters/non-voters\n",
    "\n",
    "# Simulate utility of doing a 0d10m survey \n",
    "\n",
    "    s = voted*mu_s_v + voted*sigma_s_v*rand_set0 + (1-voted)*mu_s_nv + (1-voted)*sigma_s_nv*rand_set0\n",
    "\n",
    "# Values of survey incentives (relative to 0d10m)\n",
    "# XdYm = X dollars and Y min\n",
    "\n",
    "    D_0d10m_v = 0\n",
    "    D_0d5m_v = timeval_v*5/60\n",
    "    D_10d5m_v = 10+timeval_v*5/60\n",
    "    D_10d10m_v = 10\n",
    "    D_0d10m_nv = 0\n",
    "    D_0d5m_nv = timeval_nv*5/60\n",
    "    D_10d5m_nv = 10+timeval_nv*5/60\n",
    "    D_10d10m_nv = 10\n",
    "    \n",
    "# Extra incentive if say \"not vote\"\n",
    "# 5m survey: +1m + $5 \n",
    "# 10m survey: -8m\n",
    "\n",
    "    I_5d1m_v = 5-timeval_v*1/60\n",
    "    I_8m_v = timeval_v*8/60\n",
    "    I_5d1m_nv = 5-timeval_nv*1/60\n",
    "    I_8m_nv = timeval_nv*8/60\n",
    "\n",
    "# Lying if asked\n",
    "\n",
    "# utilVotingQuestion = utility you get from being asked one time (max of lie or not lie)\n",
    "\n",
    "    wouldLieIfAsked = voted*(sn-L>sv) + (1-voted)*(sv-L>sn)\n",
    "    utilVotingQuestion = voted*np.maximum(sn-L,sv) + (1-voted)*np.maximum(sv-L,sn)\n",
    "\n",
    "# Response to incentives to say \"not vote\"\n",
    "\n",
    "    wouldLieIfAsked_5d1m = voted*(sn-L+I_5d1m_v>sv) + (1-voted)*(sv-L>sn+I_5d1m_nv)\n",
    "    wouldLieIfAsked_8m = voted*(sn-L+I_8m_v>sv) + (1-voted)*(sv-L>sn+I_8m_nv)\n",
    "\n",
    "# Compute the Moments\n",
    "\n",
    "# Utility from Doing Survey\n",
    "\n",
    "# NF = no flyer, F = flyer, FV= flyer + voting, OO = opt-out, OOV = opt-out + voting\n",
    "\n",
    "# anticipated utility from doing survey and voting survey (VF or I)\n",
    "    util_svyOnly_0d5m = s + voted*D_0d5m_v + (1-voted)*D_0d5m_nv\n",
    "    util_svyPlusVotingQues_0d5m = util_svyOnly_0d5m + utilVotingQuestion\n",
    "    util_svyOnly_10d10m = s + voted*D_10d10m_v + (1-voted)*D_10d10m_nv\n",
    "    util_svyPlusVotingQues_10d10m = util_svyOnly_10d10m + utilVotingQuestion\n",
    "    util_svyOnly_10d5m = s + voted*D_10d5m_v + (1-voted)*D_10d5m_nv\n",
    "    util_svyPlusVotingQues_10d5m = util_svyOnly_10d5m + utilVotingQuestion\n",
    "\n",
    "# If asked, do survey if greater than the social pressure cost\n",
    "\n",
    "# NI = not informed that survey is about voting, I=informed (VF or I)\n",
    "    doesSvyIfAsked_NI_0d5m = util_svyOnly_0d5m > -S_svy\n",
    "    doesSvyIfAsked_I_0d5m = util_svyPlusVotingQues_0d5m > -S_svy\n",
    "    doesSvyIfAsked_NI_10d10m = util_svyOnly_10d10m > -S_svy\n",
    "    doesSvyIfAsked_I_10d10m = util_svyPlusVotingQues_10d10m > -S_svy\n",
    "    doesSvyIfAsked_NI_10d5m = util_svyOnly_10d5m > -S_svy\n",
    "    doesSvyIfAsked_I_10d5m = util_svyPlusVotingQues_10d5m > -S_svy\n",
    "\n",
    "# Anticipated utility given you are asked to do the survey\n",
    "    anticipatedUtil_Svy_NI_0d5m = np.maximum(util_svyOnly_0d5m,-S_svy)\n",
    "    anticipatedUtil_Svy_I_0d5m = np.maximum(util_svyPlusVotingQues_0d5m,-S_svy)\n",
    "    anticipatedUtil_Svy_NI_10d10m = np.maximum(util_svyOnly_10d10m,-S_svy)\n",
    "    anticipatedUtil_Svy_I_10d10m = np.maximum(util_svyPlusVotingQues_10d10m,-S_svy)\n",
    "    anticipatedUtil_Svy_NI_10d5m = np.maximum(util_svyOnly_10d5m,-S_svy)\n",
    "    anticipatedUtil_Svy_I_10d5m = np.maximum(util_svyPlusVotingQues_10d5m,-S_svy)\n",
    "\n",
    "# Oopt-out if anticipated utility is negative\n",
    "    optsOutIfSees_OO_0d5m = anticipatedUtil_Svy_NI_0d5m < 0\n",
    "    optsOutIfSees_OOV_0d5m = anticipatedUtil_Svy_I_0d5m < 0\n",
    "    optsOutIfSees_OO_10d10m = anticipatedUtil_Svy_NI_10d10m < 0\n",
    "    optsOutIfSees_OOV_10d10m = anticipatedUtil_Svy_I_10d10m < 0\n",
    "    optsOutIfSees_OO_10d5m = anticipatedUtil_Svy_NI_10d5m < 0\n",
    "    optsOutIfSees_OOV_10d5m = anticipatedUtil_Svy_I_10d5m < 0\n",
    "\n",
    "# Choosing probability of being at home is bounded between 0 and 1\n",
    "\n",
    "    hStar_F_0d5m = np.maximum(0,np.minimum(1,h0+eta*anticipatedUtil_Svy_NI_0d5m))\n",
    "    hStar_FV_0d5m = np.maximum(0,np.minimum(1,h0+eta*anticipatedUtil_Svy_I_0d5m))\n",
    "    hStar_F_10d10m = np.maximum(0,np.minimum(1,h0+eta*anticipatedUtil_Svy_NI_10d10m))\n",
    "    hStar_FV_10d10m = np.maximum(0,np.minimum(1,h0+eta*anticipatedUtil_Svy_I_10d10m))\n",
    "    hStar_F_10d5m = np.maximum(0,np.minimum(1,h0+eta*anticipatedUtil_Svy_NI_10d5m))\n",
    "    hStar_FV_10d5m = np.maximum(0,np.minimum(1,h0+eta*anticipatedUtil_Svy_I_10d5m))\n",
    "\n",
    "# Separate Voters and Nonvoters. Split into separate vectors of voters and non-voter vectors \n",
    "# note: they will be of different length\n",
    "\n",
    "# Voters\n",
    "    hStar_F_0d5m_v = np.take(hStar_F_0d5m,voterIndex)\n",
    "    hStar_FV_0d5m_v = np.take(hStar_FV_0d5m,voterIndex)\n",
    "    doesSvyIfAsked_NI_0d5m_v = np.take(doesSvyIfAsked_NI_0d5m,voterIndex)\n",
    "    doesSvyIfAsked_I_0d5m_v = np.take(doesSvyIfAsked_I_0d5m,voterIndex)\n",
    "    optsOutIfSees_OO_0d5m_v= np.take(optsOutIfSees_OO_0d5m,voterIndex)\n",
    "    optsOutIfSees_OOV_0d5m_v= np.take(optsOutIfSees_OOV_0d5m,voterIndex)\n",
    "\n",
    "    hStar_F_10d10m_v = np.take(hStar_F_10d10m,voterIndex)\n",
    "    hStar_FV_10d10m_v = np.take(hStar_FV_10d10m,voterIndex)\n",
    "    doesSvyIfAsked_NI_10d10m_v = np.take(doesSvyIfAsked_NI_10d10m,voterIndex)\n",
    "    doesSvyIfAsked_I_10d10m_v = np.take(doesSvyIfAsked_I_10d10m,voterIndex)\n",
    "    optsOutIfSees_OO_10d10m_v= np.take(optsOutIfSees_OO_10d10m,voterIndex)\n",
    "    optsOutIfSees_OOV_10d10m_v= np.take(optsOutIfSees_OOV_10d10m,voterIndex)\n",
    "    \n",
    "    hStar_F_10d5m_v = np.take(hStar_F_10d5m,voterIndex)\n",
    "    hStar_FV_10d5m_v = np.take(hStar_FV_10d5m,voterIndex)\n",
    "    doesSvyIfAsked_NI_10d5m_v = np.take(doesSvyIfAsked_NI_10d5m,voterIndex)\n",
    "    doesSvyIfAsked_I_10d5m_v = np.take(doesSvyIfAsked_I_10d5m,voterIndex)\n",
    "    optsOutIfSees_OO_10d5m_v= np.take(optsOutIfSees_OO_10d5m,voterIndex)\n",
    "    optsOutIfSees_OOV_10d5m_v= np.take(optsOutIfSees_OOV_10d5m,voterIndex)\n",
    "\n",
    "    wouldLieIfAsked_v = np.take(wouldLieIfAsked,voterIndex)\n",
    "    wouldLieIfAsked_5d1m_v = np.take(wouldLieIfAsked_5d1m,voterIndex)\n",
    "    wouldLieIfAsked_8m_v= np.take(wouldLieIfAsked_8m,voterIndex)\n",
    "\n",
    "# Non-voters\n",
    "    hStar_F_0d5m_nv = np.take(hStar_F_0d5m,nonvoterIndex)\n",
    "    hStar_FV_0d5m_nv = np.take(hStar_FV_0d5m,nonvoterIndex)\n",
    "    doesSvyIfAsked_NI_0d5m_nv = np.take(doesSvyIfAsked_NI_0d5m,nonvoterIndex)\n",
    "    doesSvyIfAsked_I_0d5m_nv = np.take(doesSvyIfAsked_I_0d5m,nonvoterIndex)\n",
    "    optsOutIfSees_OO_0d5m_nv = np.take(optsOutIfSees_OO_0d5m,nonvoterIndex)\n",
    "    optsOutIfSees_OOV_0d5m_nv = np.take(optsOutIfSees_OOV_0d5m,nonvoterIndex)\n",
    "\n",
    "    hStar_F_10d10m_nv = np.take(hStar_F_10d10m,nonvoterIndex)\n",
    "    hStar_FV_10d10m_nv = np.take(hStar_FV_10d10m,nonvoterIndex)\n",
    "    doesSvyIfAsked_NI_10d10m_nv = np.take(doesSvyIfAsked_NI_10d10m,nonvoterIndex)\n",
    "    doesSvyIfAsked_I_10d10m_nv = np.take(doesSvyIfAsked_I_10d10m,nonvoterIndex)\n",
    "    optsOutIfSees_OO_10d10m_nv = np.take(optsOutIfSees_OO_10d10m,nonvoterIndex)\n",
    "    optsOutIfSees_OOV_10d10m_nv = np.take(optsOutIfSees_OOV_10d10m,nonvoterIndex)\n",
    "\n",
    "    hStar_F_10d5m_nv = np.take(hStar_F_10d5m,nonvoterIndex)\n",
    "    hStar_FV_10d5m_nv = np.take(hStar_FV_10d5m,nonvoterIndex)\n",
    "    doesSvyIfAsked_NI_10d5m_nv = np.take(doesSvyIfAsked_NI_10d5m,nonvoterIndex)\n",
    "    doesSvyIfAsked_I_10d5m_nv = np.take(doesSvyIfAsked_I_10d5m,nonvoterIndex)\n",
    "    optsOutIfSees_OO_10d5m_nv = np.take(optsOutIfSees_OO_10d5m,nonvoterIndex)\n",
    "    optsOutIfSees_OOV_10d5m_nv = np.take(optsOutIfSees_OOV_10d5m,nonvoterIndex)\n",
    "\n",
    "    wouldLieIfAsked_nv = np.take(wouldLieIfAsked,nonvoterIndex)\n",
    "    wouldLieIfAsked_5d1m_nv = np.take(wouldLieIfAsked_5d1m,nonvoterIndex)\n",
    "    wouldLieIfAsked_8m_nv = np.take(wouldLieIfAsked_8m,nonvoterIndex)\n",
    "\n",
    "# Disaggregated Moments\n",
    "\n",
    "# !!! Voters !!!\n",
    "\n",
    "# PH = probability of being at home\n",
    "    PH_NF_0d5m_v = h0_v\n",
    "    PH_F_0d5m_v  = (1-r_v)*h0_v + r_v*np.mean(hStar_F_0d5m_v)\n",
    "    PH_FV_0d5m_v = (1-r_v)*h0_v + r_v*np.mean(hStar_FV_0d5m_v)\n",
    "    PH_OO_0d5m_v = (1-r_v)*h0_v + r_v*np.mean((1-optsOutIfSees_OO_0d5m_v)*hStar_F_0d5m_v)\n",
    "    PH_OOV_0d5m_v = (1-r_v)*h0_v + r_v*np.mean((1-optsOutIfSees_OOV_0d5m_v)*hStar_FV_0d5m_v)\n",
    "\n",
    "    PH_NF_10d10m_v = h0_v\n",
    "    PH_F_10d10m_v = (1-r_v)*h0_v + r_v*np.mean(hStar_F_10d10m_v)\n",
    "    PH_FV_10d10m_v = (1-r_v)*h0_v + r_v*np.mean(hStar_FV_10d10m_v)\n",
    "    PH_OO_10d10m_v = (1-r_v)*h0_v + r_v*np.mean((1-optsOutIfSees_OO_10d10m_v)*hStar_F_10d10m_v)\n",
    "    PH_OOV_10d10m_v = (1-r_v)*h0_v + r_v*np.mean((1-optsOutIfSees_OOV_10d10m_v)*hStar_FV_10d10m_v)\n",
    "\n",
    "    PH_NF_10d5m_v = h0_v\n",
    "    PH_F_10d5m_v = (1-r_v)*h0_v + r_v*np.mean(hStar_F_10d5m_v)\n",
    "    PH_FV_10d5m_v = (1-r_v)*h0_v + r_v*np.mean(hStar_FV_10d5m_v)\n",
    "    PH_OO_10d5m_v = (1-r_v)*h0_v + r_v*np.mean((1-optsOutIfSees_OO_10d5m_v)*hStar_F_10d5m_v) \n",
    "    PH_OOV_10d5m_v = (1-r_v)*h0_v + r_v*np.mean((1-optsOutIfSees_OOV_10d5m_v)*hStar_FV_10d5m_v)\n",
    "\n",
    "\n",
    "# PSV = unconditional prob of doing the survey (not cond on opening door). PSV < PH mechanically\n",
    "\n",
    "# 0d5m\n",
    "    PSV_NF_NI_0d5m_v = h0_v*np.mean(doesSvyIfAsked_NI_0d5m_v)\n",
    "    PSV_NF_I_0d5m_v = h0_v*np.mean(doesSvyIfAsked_I_0d5m_v)\n",
    "\n",
    "    PSV_F_NI_0d5m_v = (1-r_v)*PSV_NF_NI_0d5m_v + r_v*np.mean(hStar_F_0d5m_v*doesSvyIfAsked_NI_0d5m_v)\n",
    "    PSV_F_I_0d5m_v = (1-r_v)*PSV_NF_I_0d5m_v + r_v*np.mean(hStar_F_0d5m_v*doesSvyIfAsked_I_0d5m_v)\n",
    "\n",
    "    PSV_FV_NI_0d5m_v = (1-r_v)*PSV_NF_NI_0d5m_v + r_v*np.mean(hStar_FV_0d5m_v*doesSvyIfAsked_I_0d5m_v)\n",
    "    PSV_FV_I_0d5m_v = (1-r_v)*PSV_NF_I_0d5m_v + r_v*np.mean(hStar_FV_0d5m_v*doesSvyIfAsked_I_0d5m_v)\n",
    "\n",
    "    PSV_OO_NI_0d5m_v = (1-r_v)*PSV_NF_NI_0d5m_v + r_v*np.mean((1-optsOutIfSees_OO_0d5m_v)*hStar_F_0d5m_v*doesSvyIfAsked_NI_0d5m_v)\n",
    "    PSV_OO_I_0d5m_v = (1-r_v)*PSV_NF_I_0d5m_v + r_v*np.mean((1-optsOutIfSees_OO_0d5m_v)*hStar_F_0d5m_v*doesSvyIfAsked_I_0d5m_v)\n",
    "\n",
    "    PSV_OOV_NI_0d5m_v = (1-r_v)*PSV_NF_NI_0d5m_v + r_v*np.mean((1-optsOutIfSees_OOV_0d5m_v)*hStar_FV_0d5m_v*doesSvyIfAsked_I_0d5m_v)\n",
    "    PSV_OOV_I_0d5m_v = (1-r_v)*PSV_NF_I_0d5m_v + r_v*np.mean((1-optsOutIfSees_OOV_0d5m_v)*hStar_FV_0d5m_v*doesSvyIfAsked_I_0d5m_v)\n",
    "\n",
    "# 10d10m\n",
    "    PSV_NF_NI_10d10m_v = h0_v*np.mean(doesSvyIfAsked_NI_10d10m_v)\n",
    "    PSV_NF_I_10d10m_v = h0_v*np.mean(doesSvyIfAsked_I_10d10m_v)\n",
    " \n",
    "    PSV_F_NI_10d10m_v = (1-r_v)*PSV_NF_NI_10d10m_v + r_v*np.mean(hStar_F_10d10m_v*doesSvyIfAsked_NI_10d10m_v)\n",
    "    PSV_F_I_10d10m_v = (1-r_v)*PSV_NF_I_10d10m_v + r_v*np.mean(hStar_F_10d10m_v*doesSvyIfAsked_I_10d10m_v)\n",
    " \n",
    "    PSV_FV_NI_10d10m_v = (1-r_v)*PSV_NF_NI_10d10m_v + r_v*np.mean(hStar_FV_10d10m_v*doesSvyIfAsked_I_10d10m_v)\n",
    "    PSV_FV_I_10d10m_v = (1-r_v)*PSV_NF_I_10d10m_v + r_v*np.mean(hStar_FV_10d10m_v*doesSvyIfAsked_I_10d10m_v)\n",
    " \n",
    "    PSV_OO_NI_10d10m_v = (1-r_v)*PSV_NF_NI_10d10m_v + r_v*np.mean((1-optsOutIfSees_OO_10d10m_v)*hStar_F_10d10m_v*doesSvyIfAsked_NI_10d10m_v)\n",
    "    PSV_OO_I_10d10m_v = (1-r_v)*PSV_NF_I_10d10m_v + r_v*np.mean((1-optsOutIfSees_OO_10d10m_v)*hStar_F_10d10m_v*doesSvyIfAsked_I_10d10m_v)\n",
    " \n",
    "    PSV_OOV_NI_10d10m_v = (1-r_v)*PSV_NF_NI_10d10m_v + r_v*np.mean((1-optsOutIfSees_OOV_10d10m_v)*hStar_FV_10d10m_v*doesSvyIfAsked_I_10d10m_v)\n",
    "    PSV_OOV_I_10d10m_v = (1-r_v)*PSV_NF_I_10d10m_v + r_v*np.mean((1-optsOutIfSees_OOV_10d10m_v)*hStar_FV_10d10m_v*doesSvyIfAsked_I_10d10m_v)\n",
    "\n",
    "# 10d5m\n",
    "    PSV_NF_NI_10d5m_v = h0_v*np.mean(doesSvyIfAsked_NI_10d5m_v)\n",
    "    PSV_NF_I_10d5m_v = h0_v*np.mean(doesSvyIfAsked_I_10d5m_v)\n",
    " \n",
    "    PSV_F_NI_10d5m_v = (1-r_v)*PSV_NF_NI_10d5m_v + r_v*np.mean(hStar_F_10d5m_v*doesSvyIfAsked_NI_10d5m_v)\n",
    "    PSV_F_I_10d5m_v = (1-r_v)*PSV_NF_I_10d5m_v + r_v*np.mean(hStar_F_10d5m_v*doesSvyIfAsked_I_10d5m_v)\n",
    " \n",
    "    PSV_FV_NI_10d5m_v = (1-r_v)*PSV_NF_NI_10d5m_v + r_v*np.mean(hStar_FV_10d5m_v*doesSvyIfAsked_I_10d5m_v)\n",
    "    PSV_FV_I_10d5m_v = (1-r_v)*PSV_NF_I_10d5m_v + r_v*np.mean(hStar_FV_10d5m_v*doesSvyIfAsked_I_10d5m_v)\n",
    " \n",
    "    PSV_OO_NI_10d5m_v = (1-r_v)*PSV_NF_NI_10d5m_v + r_v*np.mean((1-optsOutIfSees_OO_10d5m_v)*hStar_F_10d5m_v*doesSvyIfAsked_NI_10d5m_v)\n",
    "    PSV_OO_I_10d5m_v = (1-r_v)*PSV_NF_I_10d5m_v + r_v*np.mean((1-optsOutIfSees_OO_10d5m_v)*hStar_F_10d5m_v*doesSvyIfAsked_I_10d5m_v)\n",
    " \n",
    "    PSV_OOV_NI_10d5m_v = (1-r_v)*PSV_NF_NI_10d5m_v + r_v*np.mean((1-optsOutIfSees_OOV_10d5m_v)*hStar_FV_10d5m_v*doesSvyIfAsked_I_10d5m_v)\n",
    "    PSV_OOV_I_10d5m_v = (1-r_v)*PSV_NF_I_10d5m_v + r_v*np.mean((1-optsOutIfSees_OOV_10d5m_v)*hStar_FV_10d5m_v*doesSvyIfAsked_I_10d5m_v)\n",
    "\n",
    "\n",
    "# POO=prob of opting out (not conditional on seeing flyer)\n",
    "# Scaled by baseline likelihood of being at home\n",
    "    POO_OO_0d5m_v =  h0_v*r_v*np.mean(optsOutIfSees_OO_0d5m_v)\n",
    "    POO_OOV_0d5m_v = h0_v*r_v*np.mean(optsOutIfSees_OOV_0d5m_v)\n",
    "\n",
    "    POO_OO_10d10m_v =  h0_v*r_v*np.mean(optsOutIfSees_OO_10d10m_v)\n",
    "    POO_OOV_10d10m_v = h0_v*r_v*np.mean(optsOutIfSees_OOV_10d10m_v)\n",
    "\n",
    "    POO_OO_10d5m_v =  h0_v*r_v*np.mean(optsOutIfSees_OO_10d5m_v)\n",
    "    POO_OOV_10d5m_v = h0_v*r_v*np.mean(optsOutIfSees_OOV_10d5m_v)\n",
    "\n",
    "\n",
    "# Empirical moments are total lying in treatments / total doing survey in treatments\n",
    "\n",
    "# PSVL = unconditional percent who do survey and lie \n",
    "# No flyer treatment only, simplifies later code\n",
    "\n",
    "# PL=cond on agreeing to do the survey, did you lie?\n",
    "# Incentive to lie is a surprise later (doesn't affect PH or PSV)\n",
    "\n",
    "# 0d5m, 5d1m incentive\n",
    "    PSVL_NF_NI_0d5m_v = np.mean(h0_v*doesSvyIfAsked_NI_0d5m_v*wouldLieIfAsked_v)\n",
    "    PSVL_NF_I_0d5m_v = np.mean(h0_v*doesSvyIfAsked_I_0d5m_v*wouldLieIfAsked_v)\n",
    "    PSVL_NF_NI_0d5m_5d1m_v = np.mean(h0_v*doesSvyIfAsked_NI_0d5m_v*wouldLieIfAsked_5d1m_v)\n",
    "    PSVL_NF_I_0d5m_5d1m_v = np.mean(h0_v*doesSvyIfAsked_I_0d5m_v*wouldLieIfAsked_5d1m_v)\n",
    "\n",
    "    PSVL_F_NI_0d5m_v = (1-r_v)*PSVL_NF_NI_0d5m_v + r_v*np.mean(hStar_F_0d5m_v*doesSvyIfAsked_NI_0d5m_v*wouldLieIfAsked_v)\n",
    "    PSVL_F_I_0d5m_v = (1-r_v)*PSVL_NF_I_0d5m_v + r_v*np.mean(hStar_F_0d5m_v*doesSvyIfAsked_I_0d5m_v*wouldLieIfAsked_v)\n",
    "    PSVL_F_NI_0d5m_5d1m_v = (1-r_v)*PSVL_NF_NI_0d5m_5d1m_v + r_v*np.mean(hStar_F_0d5m_v*doesSvyIfAsked_NI_0d5m_v*wouldLieIfAsked_5d1m_v)\n",
    "    PSVL_F_I_0d5m_5d1m_v = (1-r_v)*PSVL_NF_I_0d5m_5d1m_v + r_v*np.mean(hStar_F_0d5m_v*doesSvyIfAsked_I_0d5m_v*wouldLieIfAsked_5d1m_v)\n",
    "\n",
    "    PSVL_FV_NI_0d5m_v = (1-r_v)*PSVL_NF_NI_0d5m_v + r_v*np.mean(hStar_FV_0d5m_v*doesSvyIfAsked_I_0d5m_v*wouldLieIfAsked_v)\n",
    "    PSVL_FV_I_0d5m_v = (1-r_v)*PSVL_NF_I_0d5m_v + r_v*np.mean(hStar_FV_0d5m_v*doesSvyIfAsked_I_0d5m_v*wouldLieIfAsked_v)\n",
    "    PSVL_FV_NI_0d5m_5d1m_v = (1-r_v)*PSVL_NF_NI_0d5m_5d1m_v + r_v*np.mean(hStar_FV_0d5m_v*doesSvyIfAsked_I_0d5m_v*wouldLieIfAsked_5d1m_v)\n",
    "    PSVL_FV_I_0d5m_5d1m_v = (1-r_v)*PSVL_NF_I_0d5m_5d1m_v + r_v*np.mean(hStar_FV_0d5m_v*doesSvyIfAsked_I_0d5m_v*wouldLieIfAsked_5d1m_v)\n",
    "\n",
    "    PSVL_OO_NI_0d5m_v = (1-r_v)*PSVL_NF_NI_0d5m_v + r_v*np.mean((1-optsOutIfSees_OO_0d5m_v)*hStar_F_0d5m_v*doesSvyIfAsked_NI_0d5m_v*wouldLieIfAsked_v)\n",
    "    PSVL_OO_I_0d5m_v = (1-r_v)*PSVL_NF_I_0d5m_v + r_v*np.mean((1-optsOutIfSees_OO_0d5m_v)*hStar_F_0d5m_v*doesSvyIfAsked_I_0d5m_v*wouldLieIfAsked_v)\n",
    "    PSVL_OO_NI_0d5m_5d1m_v = (1-r_v)*PSVL_NF_NI_0d5m_5d1m_v + r_v*np.mean((1-optsOutIfSees_OO_0d5m_v)*hStar_F_0d5m_v*doesSvyIfAsked_NI_0d5m_v*wouldLieIfAsked_5d1m_v)\n",
    "    PSVL_OO_I_0d5m_5d1m_v = (1-r_v)*PSVL_NF_I_0d5m_5d1m_v + r_v*np.mean((1-optsOutIfSees_OO_0d5m_v)*hStar_F_0d5m_v*doesSvyIfAsked_I_0d5m_v*wouldLieIfAsked_5d1m_v)\n",
    "\n",
    "    PSVL_OOV_NI_0d5m_v = (1-r_v)*PSVL_NF_NI_0d5m_v + r_v*np.mean((1-optsOutIfSees_OOV_0d5m_v)*hStar_FV_0d5m_v*doesSvyIfAsked_I_0d5m_v*wouldLieIfAsked_v)\n",
    "    PSVL_OOV_I_0d5m_v = (1-r_v)*PSVL_NF_I_0d5m_v + r_v*np.mean((1-optsOutIfSees_OOV_0d5m_v)*hStar_FV_0d5m_v*doesSvyIfAsked_I_0d5m_v*wouldLieIfAsked_v)\n",
    "    PSVL_OOV_NI_0d5m_5d1m_v = (1-r_v)*PSVL_NF_NI_0d5m_5d1m_v + r_v*np.mean((1-optsOutIfSees_OOV_0d5m_v)*hStar_FV_0d5m_v*doesSvyIfAsked_I_0d5m_v*wouldLieIfAsked_5d1m_v)\n",
    "    PSVL_OOV_I_0d5m_5d1m_v = (1-r_v)*PSVL_NF_I_0d5m_5d1m_v + r_v*np.mean((1-optsOutIfSees_OOV_0d5m_v)*hStar_FV_0d5m_v*doesSvyIfAsked_I_0d5m_v*wouldLieIfAsked_5d1m_v)\n",
    "\n",
    "# 10d10m, 8m incentive\n",
    "    PSVL_NF_NI_10d10m_v = np.mean(h0_v*doesSvyIfAsked_NI_10d10m_v*wouldLieIfAsked_v)\n",
    "    PSVL_NF_I_10d10m_v = np.mean(h0_v*doesSvyIfAsked_I_10d10m_v*wouldLieIfAsked_v)\n",
    "    PSVL_NF_NI_10d10m_8m_v = np.mean(h0_v*doesSvyIfAsked_NI_10d10m_v*wouldLieIfAsked_8m_v)\n",
    "    PSVL_NF_I_10d10m_8m_v = np.mean(h0_v*doesSvyIfAsked_I_10d10m_v*wouldLieIfAsked_8m_v)\n",
    "\n",
    "    PSVL_F_NI_10d10m_v = (1-r_v)*PSVL_NF_NI_10d10m_v + r_v*np.mean(hStar_F_10d10m_v*doesSvyIfAsked_NI_10d10m_v*wouldLieIfAsked_v)\n",
    "    PSVL_F_I_10d10m_v = (1-r_v)*PSVL_NF_I_10d10m_v + r_v*np.mean(hStar_F_10d10m_v*doesSvyIfAsked_I_10d10m_v*wouldLieIfAsked_v)\n",
    "    PSVL_F_NI_10d10m_8m_v = (1-r_v)*PSVL_NF_NI_10d10m_8m_v + r_v*np.mean(hStar_F_10d10m_v*doesSvyIfAsked_NI_10d10m_v*wouldLieIfAsked_8m_v)\n",
    "    PSVL_F_I_10d10m_8m_v = (1-r_v)*PSVL_NF_I_10d10m_8m_v + r_v*np.mean(hStar_F_10d10m_v*doesSvyIfAsked_I_10d10m_v*wouldLieIfAsked_8m_v)\n",
    " \n",
    "    PSVL_FV_NI_10d10m_v = (1-r_v)*PSVL_NF_NI_10d10m_v + r_v*np.mean(hStar_FV_10d10m_v*doesSvyIfAsked_I_10d10m_v*wouldLieIfAsked_v)\n",
    "    PSVL_FV_I_10d10m_v = (1-r_v)*PSVL_NF_I_10d10m_v + r_v*np.mean(hStar_FV_10d10m_v*doesSvyIfAsked_I_10d10m_v*wouldLieIfAsked_v)\n",
    "    PSVL_FV_NI_10d10m_8m_v = (1-r_v)*PSVL_NF_NI_10d10m_8m_v + r_v*np.mean(hStar_FV_10d10m_v*doesSvyIfAsked_I_10d10m_v*wouldLieIfAsked_8m_v)\n",
    "    PSVL_FV_I_10d10m_8m_v = (1-r_v)*PSVL_NF_I_10d10m_8m_v + r_v*np.mean(hStar_FV_10d10m_v*doesSvyIfAsked_I_10d10m_v*wouldLieIfAsked_8m_v)\n",
    " \n",
    "    PSVL_OO_NI_10d10m_v = (1-r_v)*PSVL_NF_NI_10d10m_v + r_v*np.mean((1-optsOutIfSees_OO_10d10m_v)*hStar_F_10d10m_v*doesSvyIfAsked_NI_10d10m_v*wouldLieIfAsked_v)\n",
    "    PSVL_OO_I_10d10m_v = (1-r_v)*PSVL_NF_I_10d10m_v + r_v*np.mean((1-optsOutIfSees_OO_10d10m_v)*hStar_F_10d10m_v*doesSvyIfAsked_I_10d10m_v*wouldLieIfAsked_v)\n",
    "    PSVL_OO_NI_10d10m_8m_v = (1-r_v)*PSVL_NF_NI_10d10m_8m_v + r_v*np.mean((1-optsOutIfSees_OO_10d10m_v)*hStar_F_10d10m_v*doesSvyIfAsked_NI_10d10m_v*wouldLieIfAsked_8m_v)\n",
    "    PSVL_OO_I_10d10m_8m_v = (1-r_v)*PSVL_NF_I_10d10m_8m_v + r_v*np.mean((1-optsOutIfSees_OO_10d10m_v)*hStar_F_10d10m_v*doesSvyIfAsked_I_10d10m_v*wouldLieIfAsked_8m_v)\n",
    " \n",
    "    PSVL_OOV_NI_10d10m_v = (1-r_v)*PSVL_NF_NI_10d10m_v + r_v*np.mean((1-optsOutIfSees_OOV_10d10m_v)*hStar_FV_10d10m_v*doesSvyIfAsked_I_10d10m_v*wouldLieIfAsked_v)\n",
    "    PSVL_OOV_I_10d10m_v = (1-r_v)*PSVL_NF_I_10d10m_v + r_v*np.mean((1-optsOutIfSees_OOV_10d10m_v)*hStar_FV_10d10m_v*doesSvyIfAsked_I_10d10m_v*wouldLieIfAsked_v)\n",
    "    PSVL_OOV_NI_10d10m_8m_v = (1-r_v)*PSVL_NF_NI_10d10m_8m_v + r_v*np.mean((1-optsOutIfSees_OOV_10d10m_v)*hStar_FV_10d10m_v*doesSvyIfAsked_I_10d10m_v*wouldLieIfAsked_8m_v)\n",
    "    PSVL_OOV_I_10d10m_8m_v = (1-r_v)*PSVL_NF_I_10d10m_8m_v + r_v*np.mean((1-optsOutIfSees_OOV_10d10m_v)*hStar_FV_10d10m_v*doesSvyIfAsked_I_10d10m_v*wouldLieIfAsked_8m_v)\n",
    "\n",
    "# 10d5m, 5d1m incentive\n",
    "    PSVL_NF_NI_10d5m_v = np.mean(h0_v*doesSvyIfAsked_NI_10d5m_v*wouldLieIfAsked_v)\n",
    "    PSVL_NF_I_10d5m_v = np.mean(h0_v*doesSvyIfAsked_I_10d5m_v*wouldLieIfAsked_v)\n",
    "    PSVL_NF_NI_10d5m_5d1m_v = np.mean(h0_v*doesSvyIfAsked_NI_10d5m_v*wouldLieIfAsked_5d1m_v)\n",
    "    PSVL_NF_I_10d5m_5d1m_v = np.mean(h0_v*doesSvyIfAsked_I_10d5m_v*wouldLieIfAsked_5d1m_v)\n",
    " \n",
    "    PSVL_F_NI_10d5m_v = (1-r_v)*PSVL_NF_NI_10d5m_v + r_v*np.mean(hStar_F_10d5m_v*doesSvyIfAsked_NI_10d5m_v*wouldLieIfAsked_v)\n",
    "    PSVL_F_I_10d5m_v = (1-r_v)*PSVL_NF_I_10d5m_v + r_v*np.mean(hStar_F_10d5m_v*doesSvyIfAsked_I_10d5m_v*wouldLieIfAsked_v)\n",
    "    PSVL_F_NI_10d5m_5d1m_v = (1-r_v)*PSVL_NF_NI_10d5m_5d1m_v + r_v*np.mean(hStar_F_10d5m_v*doesSvyIfAsked_NI_10d5m_v*wouldLieIfAsked_5d1m_v)\n",
    "    PSVL_F_I_10d5m_5d1m_v = (1-r_v)*PSVL_NF_I_10d5m_5d1m_v + r_v*np.mean(hStar_F_10d5m_v*doesSvyIfAsked_I_10d5m_v*wouldLieIfAsked_5d1m_v)\n",
    " \n",
    "    PSVL_FV_NI_10d5m_v = (1-r_v)*PSVL_NF_NI_10d5m_v + r_v*np.mean(hStar_FV_10d5m_v*doesSvyIfAsked_I_10d5m_v*wouldLieIfAsked_v)\n",
    "    PSVL_FV_I_10d5m_v = (1-r_v)*PSVL_NF_I_10d5m_v + r_v*np.mean(hStar_FV_10d5m_v*doesSvyIfAsked_I_10d5m_v*wouldLieIfAsked_v)\n",
    "    PSVL_FV_NI_10d5m_5d1m_v = (1-r_v)*PSVL_NF_NI_10d5m_5d1m_v + r_v*np.mean(hStar_FV_10d5m_v*doesSvyIfAsked_I_10d5m_v*wouldLieIfAsked_5d1m_v)\n",
    "    PSVL_FV_I_10d5m_5d1m_v = (1-r_v)*PSVL_NF_I_10d5m_5d1m_v + r_v*np.mean(hStar_FV_10d5m_v*doesSvyIfAsked_I_10d5m_v*wouldLieIfAsked_5d1m_v)\n",
    " \n",
    "    PSVL_OO_NI_10d5m_v = (1-r_v)*PSVL_NF_NI_10d5m_v + r_v*np.mean((1-optsOutIfSees_OO_10d5m_v)*hStar_F_10d5m_v*doesSvyIfAsked_NI_10d5m_v*wouldLieIfAsked_v)\n",
    "    PSVL_OO_I_10d5m_v = (1-r_v)*PSVL_NF_I_10d5m_v + r_v*np.mean((1-optsOutIfSees_OO_10d5m_v)*hStar_F_10d5m_v*doesSvyIfAsked_I_10d5m_v*wouldLieIfAsked_v)\n",
    "    PSVL_OO_NI_10d5m_5d1m_v = (1-r_v)*PSVL_NF_NI_10d5m_5d1m_v + r_v*np.mean((1-optsOutIfSees_OO_10d5m_v)*hStar_F_10d5m_v*doesSvyIfAsked_NI_10d5m_v*wouldLieIfAsked_5d1m_v)\n",
    "    PSVL_OO_I_10d5m_5d1m_v = (1-r_v)*PSVL_NF_I_10d5m_5d1m_v + r_v*np.mean((1-optsOutIfSees_OO_10d5m_v)*hStar_F_10d5m_v*doesSvyIfAsked_I_10d5m_v*wouldLieIfAsked_5d1m_v)\n",
    " \n",
    "    PSVL_OOV_NI_10d5m_v = (1-r_v)*PSVL_NF_NI_10d5m_v + r_v*np.mean((1-optsOutIfSees_OOV_10d5m_v)*hStar_FV_10d5m_v*doesSvyIfAsked_I_10d5m_v*wouldLieIfAsked_v)\n",
    "    PSVL_OOV_I_10d5m_v = (1-r_v)*PSVL_NF_I_10d5m_v + r_v*np.mean((1-optsOutIfSees_OOV_10d5m_v)*hStar_FV_10d5m_v*doesSvyIfAsked_I_10d5m_v*wouldLieIfAsked_v)\n",
    "    PSVL_OOV_NI_10d5m_5d1m_v = (1-r_v)*PSVL_NF_NI_10d5m_5d1m_v + r_v*np.mean((1-optsOutIfSees_OOV_10d5m_v)*hStar_FV_10d5m_v*doesSvyIfAsked_I_10d5m_v*wouldLieIfAsked_5d1m_v)\n",
    "    PSVL_OOV_I_10d5m_5d1m_v = (1-r_v)*PSVL_NF_I_10d5m_5d1m_v + r_v*np.mean((1-optsOutIfSees_OOV_10d5m_v)*hStar_FV_10d5m_v*doesSvyIfAsked_I_10d5m_v*wouldLieIfAsked_5d1m_v)\n",
    "\n",
    "# !!! Non-voters (same as voters with _nv subscript) !!!\n",
    "\n",
    "    PH_NF_0d5m_nv = h0_nv\n",
    "    PH_F_0d5m_nv = (1-r_nv)*h0_nv + r_nv*np.mean(hStar_F_0d5m_nv)\n",
    "    PH_FV_0d5m_nv = (1-r_nv)*h0_nv + r_nv*np.mean(hStar_FV_0d5m_nv)\n",
    "    PH_OO_0d5m_nv = (1-r_nv)*h0_nv + r_nv*np.mean((1-optsOutIfSees_OO_0d5m_nv)*hStar_F_0d5m_nv)\n",
    "    PH_OOV_0d5m_nv = (1-r_nv)*h0_nv + r_nv*np.mean((1-optsOutIfSees_OOV_0d5m_nv)*hStar_FV_0d5m_nv)\n",
    "\n",
    "    PH_NF_10d10m_nv = h0_nv\n",
    "    PH_F_10d10m_nv = (1-r_nv)*h0_nv + r_nv*np.mean(hStar_F_10d10m_nv)\n",
    "    PH_FV_10d10m_nv = (1-r_nv)*h0_nv + r_nv*np.mean(hStar_FV_10d10m_nv)\n",
    "    PH_OO_10d10m_nv = (1-r_nv)*h0_nv + r_nv*np.mean((1-optsOutIfSees_OO_10d10m_nv)*hStar_F_10d10m_nv)\n",
    "    PH_OOV_10d10m_nv = (1-r_nv)*h0_nv + r_nv*np.mean((1-optsOutIfSees_OOV_10d10m_nv)*hStar_FV_10d10m_nv)\n",
    "\n",
    "    PH_NF_10d5m_nv = h0_nv\n",
    "    PH_F_10d5m_nv = (1-r_nv)*h0_nv + r_nv*np.mean(hStar_F_10d5m_nv)\n",
    "    PH_FV_10d5m_nv = (1-r_nv)*h0_nv + r_nv*np.mean(hStar_FV_10d5m_nv)\n",
    "    PH_OO_10d5m_nv = (1-r_nv)*h0_nv + r_nv*np.mean((1-optsOutIfSees_OO_10d5m_nv)*hStar_F_10d5m_nv) \n",
    "    PH_OOV_10d5m_nv = (1-r_nv)*h0_nv + r_nv*np.mean((1-optsOutIfSees_OOV_10d5m_nv)*hStar_FV_10d5m_nv)\n",
    "\n",
    "\n",
    "# PSV = unconditional prob of doing the survey (not cond on opening door). PSV < PH mechanically\n",
    "\n",
    "# 0d5m\n",
    "    PSV_NF_NI_0d5m_nv = h0_nv*np.mean(doesSvyIfAsked_NI_0d5m_nv)\n",
    "    PSV_NF_I_0d5m_nv = h0_nv*np.mean(doesSvyIfAsked_I_0d5m_nv)\n",
    "\n",
    "    PSV_F_NI_0d5m_nv = (1-r_nv)*PSV_NF_NI_0d5m_nv + r_nv*np.mean(hStar_F_0d5m_nv*doesSvyIfAsked_NI_0d5m_nv)\n",
    "    PSV_F_I_0d5m_nv = (1-r_nv)*PSV_NF_I_0d5m_nv + r_nv*np.mean(hStar_F_0d5m_nv*doesSvyIfAsked_I_0d5m_nv)\n",
    "\n",
    "    PSV_FV_NI_0d5m_nv = (1-r_nv)*PSV_NF_NI_0d5m_nv + r_nv*np.mean(hStar_FV_0d5m_nv*doesSvyIfAsked_I_0d5m_nv)\n",
    "    PSV_FV_I_0d5m_nv = (1-r_nv)*PSV_NF_I_0d5m_nv + r_nv*np.mean(hStar_FV_0d5m_nv*doesSvyIfAsked_I_0d5m_nv)\n",
    "\n",
    "    PSV_OO_NI_0d5m_nv = (1-r_nv)*PSV_NF_NI_0d5m_nv + r_nv*np.mean((1-optsOutIfSees_OO_0d5m_nv)*hStar_F_0d5m_nv*doesSvyIfAsked_NI_0d5m_nv)\n",
    "    PSV_OO_I_0d5m_nv = (1-r_nv)*PSV_NF_I_0d5m_nv + r_nv*np.mean((1-optsOutIfSees_OO_0d5m_nv)*hStar_F_0d5m_nv*doesSvyIfAsked_I_0d5m_nv)\n",
    "\n",
    "    PSV_OOV_NI_0d5m_nv = (1-r_nv)*PSV_NF_NI_0d5m_nv + r_nv*np.mean((1-optsOutIfSees_OOV_0d5m_nv)*hStar_FV_0d5m_nv*doesSvyIfAsked_I_0d5m_nv)\n",
    "    PSV_OOV_I_0d5m_nv = (1-r_nv)*PSV_NF_I_0d5m_nv + r_nv*np.mean((1-optsOutIfSees_OOV_0d5m_nv)*hStar_FV_0d5m_nv*doesSvyIfAsked_I_0d5m_nv)\n",
    "\n",
    "# 10d10m\n",
    "    PSV_NF_NI_10d10m_nv = h0_nv*np.mean(doesSvyIfAsked_NI_10d10m_nv)\n",
    "    PSV_NF_I_10d10m_nv = h0_nv*np.mean(doesSvyIfAsked_I_10d10m_nv)\n",
    "    \n",
    "    PSV_F_NI_10d10m_nv = (1-r_nv)*PSV_NF_NI_10d10m_nv + r_nv*np.mean(hStar_F_10d10m_nv*doesSvyIfAsked_NI_10d10m_nv)\n",
    "    PSV_F_I_10d10m_nv = (1-r_nv)*PSV_NF_I_10d10m_nv + r_nv*np.mean(hStar_F_10d10m_nv*doesSvyIfAsked_I_10d10m_nv)\n",
    " \n",
    "    PSV_FV_NI_10d10m_nv = (1-r_nv)*PSV_NF_NI_10d10m_nv + r_nv*np.mean(hStar_FV_10d10m_nv*doesSvyIfAsked_I_10d10m_nv)\n",
    "    PSV_FV_I_10d10m_nv = (1-r_nv)*PSV_NF_I_10d10m_nv + r_nv*np.mean(hStar_FV_10d10m_nv*doesSvyIfAsked_I_10d10m_nv)\n",
    "    \n",
    "    PSV_OO_NI_10d10m_nv = (1-r_nv)*PSV_NF_NI_10d10m_nv + r_nv*np.mean((1-optsOutIfSees_OO_10d10m_nv)*hStar_F_10d10m_nv*doesSvyIfAsked_NI_10d10m_nv)\n",
    "    PSV_OO_I_10d10m_nv = (1-r_nv)*PSV_NF_I_10d10m_nv + r_nv*np.mean((1-optsOutIfSees_OO_10d10m_nv)*hStar_F_10d10m_nv*doesSvyIfAsked_I_10d10m_nv)\n",
    " \n",
    "    PSV_OOV_NI_10d10m_nv = (1-r_nv)*PSV_NF_NI_10d10m_nv + r_nv*np.mean((1-optsOutIfSees_OOV_10d10m_nv)*hStar_FV_10d10m_nv*doesSvyIfAsked_I_10d10m_nv)\n",
    "    PSV_OOV_I_10d10m_nv = (1-r_nv)*PSV_NF_I_10d10m_nv + r_nv*np.mean((1-optsOutIfSees_OOV_10d10m_nv)*hStar_FV_10d10m_nv*doesSvyIfAsked_I_10d10m_nv)\n",
    "\n",
    "#  10d5m\n",
    "    PSV_NF_NI_10d5m_nv = h0_nv*np.mean(doesSvyIfAsked_NI_10d5m_nv)\n",
    "    PSV_NF_I_10d5m_nv = h0_nv*np.mean(doesSvyIfAsked_I_10d5m_nv)\n",
    " \n",
    "    PSV_F_NI_10d5m_nv = (1-r_nv)*PSV_NF_NI_10d5m_nv + r_nv*np.mean(hStar_F_10d5m_nv*doesSvyIfAsked_NI_10d5m_nv)\n",
    "    PSV_F_I_10d5m_nv = (1-r_nv)*PSV_NF_I_10d5m_nv + r_nv*np.mean(hStar_F_10d5m_nv*doesSvyIfAsked_I_10d5m_nv)\n",
    " \n",
    "    PSV_FV_NI_10d5m_nv = (1-r_nv)*PSV_NF_NI_10d5m_nv + r_nv*np.mean(hStar_FV_10d5m_nv*doesSvyIfAsked_I_10d5m_nv)\n",
    "    PSV_FV_I_10d5m_nv = (1-r_nv)*PSV_NF_I_10d5m_nv + r_nv*np.mean(hStar_FV_10d5m_nv*doesSvyIfAsked_I_10d5m_nv)\n",
    " \n",
    "    PSV_OO_NI_10d5m_nv = (1-r_nv)*PSV_NF_NI_10d5m_nv + r_nv*np.mean((1-optsOutIfSees_OO_10d5m_nv)*hStar_F_10d5m_nv*doesSvyIfAsked_NI_10d5m_nv)\n",
    "    PSV_OO_I_10d5m_nv = (1-r_nv)*PSV_NF_I_10d5m_nv + r_nv*np.mean((1-optsOutIfSees_OO_10d5m_nv)*hStar_F_10d5m_nv*doesSvyIfAsked_I_10d5m_nv)\n",
    " \n",
    "    PSV_OOV_NI_10d5m_nv = (1-r_nv)*PSV_NF_NI_10d5m_nv + r_nv*np.mean((1-optsOutIfSees_OOV_10d5m_nv)*hStar_FV_10d5m_nv*doesSvyIfAsked_I_10d5m_nv)\n",
    "    PSV_OOV_I_10d5m_nv = (1-r_nv)*PSV_NF_I_10d5m_nv + r_nv*np.mean((1-optsOutIfSees_OOV_10d5m_nv)*hStar_FV_10d5m_nv*doesSvyIfAsked_I_10d5m_nv)\n",
    " \n",
    " \n",
    "# POO=prob of opting out (not conditional on seeing flyer). Scaled by baseline likelihood of being at home\n",
    "    POO_OO_0d5m_nv =  h0_nv*r_nv*np.mean(optsOutIfSees_OO_0d5m_nv)\n",
    "    POO_OOV_0d5m_nv = h0_nv*r_nv*np.mean(optsOutIfSees_OOV_0d5m_nv)\n",
    " \n",
    "    POO_OO_10d10m_nv =  h0_nv*r_nv*np.mean(optsOutIfSees_OO_10d10m_nv)\n",
    "    POO_OOV_10d10m_nv = h0_nv*r_nv*np.mean(optsOutIfSees_OOV_10d10m_nv)\n",
    " \n",
    "    POO_OO_10d5m_nv =  h0_nv*r_nv*np.mean(optsOutIfSees_OO_10d5m_nv)\n",
    "    POO_OOV_10d5m_nv = h0_nv*r_nv*np.mean(optsOutIfSees_OOV_10d5m_nv)\n",
    "\n",
    "    \n",
    "# PSVL = unconditional percent who do survey and lie. No flyer treatment only\n",
    " \n",
    "# PL=cond on agreeing to do the survey, did you lie? Incentive to lie is a surprise later (doesn't affect PH or PSV)\n",
    " \n",
    "# 0d5m, 5d1m incentive\n",
    "    PSVL_NF_NI_0d5m_nv = np.mean(h0_nv*doesSvyIfAsked_NI_0d5m_nv*wouldLieIfAsked_nv)\n",
    "    PSVL_NF_I_0d5m_nv = np.mean(h0_nv*doesSvyIfAsked_I_0d5m_nv*wouldLieIfAsked_nv)\n",
    "    PSVL_NF_NI_0d5m_5d1m_nv = np.mean(h0_nv*doesSvyIfAsked_NI_0d5m_nv*wouldLieIfAsked_5d1m_nv)\n",
    "    PSVL_NF_I_0d5m_5d1m_nv = np.mean(h0_nv*doesSvyIfAsked_I_0d5m_nv*wouldLieIfAsked_5d1m_nv)\n",
    "\n",
    "    PSVL_F_NI_0d5m_nv = (1-r_nv)*PSVL_NF_NI_0d5m_nv + r_nv*np.mean(hStar_F_0d5m_nv*doesSvyIfAsked_NI_0d5m_nv*wouldLieIfAsked_nv)\n",
    "    PSVL_F_I_0d5m_nv = (1-r_nv)*PSVL_NF_I_0d5m_nv + r_nv*np.mean(hStar_F_0d5m_nv*doesSvyIfAsked_I_0d5m_nv*wouldLieIfAsked_nv)\n",
    "    PSVL_F_NI_0d5m_5d1m_nv = (1-r_nv)*PSVL_NF_NI_0d5m_5d1m_nv + r_nv*np.mean(hStar_F_0d5m_nv*doesSvyIfAsked_NI_0d5m_nv*wouldLieIfAsked_5d1m_nv)\n",
    "    PSVL_F_I_0d5m_5d1m_nv = (1-r_nv)*PSVL_NF_I_0d5m_5d1m_nv + r_nv*np.mean(hStar_F_0d5m_nv*doesSvyIfAsked_I_0d5m_nv*wouldLieIfAsked_5d1m_nv)\n",
    " \n",
    "    PSVL_FV_NI_0d5m_nv = (1-r_nv)*PSVL_NF_NI_0d5m_nv + r_nv*np.mean(hStar_FV_0d5m_nv*doesSvyIfAsked_I_0d5m_nv*wouldLieIfAsked_nv)\n",
    "    PSVL_FV_I_0d5m_nv = (1-r_nv)*PSVL_NF_I_0d5m_nv + r_nv*np.mean(hStar_FV_0d5m_nv*doesSvyIfAsked_I_0d5m_nv*wouldLieIfAsked_nv)\n",
    "    PSVL_FV_NI_0d5m_5d1m_nv = (1-r_nv)*PSVL_NF_NI_0d5m_5d1m_nv + r_nv*np.mean(hStar_FV_0d5m_nv*doesSvyIfAsked_I_0d5m_nv*wouldLieIfAsked_5d1m_nv)\n",
    "    PSVL_FV_I_0d5m_5d1m_nv = (1-r_nv)*PSVL_NF_I_0d5m_5d1m_nv + r_nv*np.mean(hStar_FV_0d5m_nv*doesSvyIfAsked_I_0d5m_nv*wouldLieIfAsked_5d1m_nv)\n",
    " \n",
    "    PSVL_OO_NI_0d5m_nv = (1-r_nv)*PSVL_NF_NI_0d5m_nv + r_nv*np.mean((1-optsOutIfSees_OO_0d5m_nv)*hStar_F_0d5m_nv*doesSvyIfAsked_NI_0d5m_nv*wouldLieIfAsked_nv)\n",
    "    PSVL_OO_I_0d5m_nv = (1-r_nv)*PSVL_NF_I_0d5m_nv + r_nv*np.mean((1-optsOutIfSees_OO_0d5m_nv)*hStar_F_0d5m_nv*doesSvyIfAsked_I_0d5m_nv*wouldLieIfAsked_nv)\n",
    "    PSVL_OO_NI_0d5m_5d1m_nv = (1-r_nv)*PSVL_NF_NI_0d5m_5d1m_nv + r_nv*np.mean((1-optsOutIfSees_OO_0d5m_nv)*hStar_F_0d5m_nv*doesSvyIfAsked_NI_0d5m_nv*wouldLieIfAsked_5d1m_nv)\n",
    "    PSVL_OO_I_0d5m_5d1m_nv = (1-r_nv)*PSVL_NF_I_0d5m_5d1m_nv + r_nv*np.mean((1-optsOutIfSees_OO_0d5m_nv)*hStar_F_0d5m_nv*doesSvyIfAsked_I_0d5m_nv*wouldLieIfAsked_5d1m_nv)\n",
    " \n",
    "    PSVL_OOV_NI_0d5m_nv = (1-r_nv)*PSVL_NF_NI_0d5m_nv + r_nv*np.mean((1-optsOutIfSees_OOV_0d5m_nv)*hStar_FV_0d5m_nv*doesSvyIfAsked_I_0d5m_nv*wouldLieIfAsked_nv)\n",
    "    PSVL_OOV_I_0d5m_nv = (1-r_nv)*PSVL_NF_I_0d5m_nv + r_nv*np.mean((1-optsOutIfSees_OOV_0d5m_nv)*hStar_FV_0d5m_nv*doesSvyIfAsked_I_0d5m_nv*wouldLieIfAsked_nv)\n",
    "    PSVL_OOV_NI_0d5m_5d1m_nv = (1-r_nv)*PSVL_NF_NI_0d5m_5d1m_nv + r_nv*np.mean((1-optsOutIfSees_OOV_0d5m_nv)*hStar_FV_0d5m_nv*doesSvyIfAsked_I_0d5m_nv*wouldLieIfAsked_5d1m_nv)\n",
    "    PSVL_OOV_I_0d5m_5d1m_nv = (1-r_nv)*PSVL_NF_I_0d5m_5d1m_nv + r_nv*np.mean((1-optsOutIfSees_OOV_0d5m_nv)*hStar_FV_0d5m_nv*doesSvyIfAsked_I_0d5m_nv*wouldLieIfAsked_5d1m_nv)\n",
    " \n",
    " \n",
    "# 10d10m, 8m incentive\n",
    "    PSVL_NF_NI_10d10m_nv = np.mean(h0_nv*doesSvyIfAsked_NI_10d10m_nv*wouldLieIfAsked_nv)\n",
    "    PSVL_NF_I_10d10m_nv = np.mean(h0_nv*doesSvyIfAsked_I_10d10m_nv*wouldLieIfAsked_nv)\n",
    "    PSVL_NF_NI_10d10m_8m_nv = np.mean(h0_nv*doesSvyIfAsked_NI_10d10m_nv*wouldLieIfAsked_8m_nv)\n",
    "    PSVL_NF_I_10d10m_8m_nv = np.mean(h0_nv*doesSvyIfAsked_I_10d10m_nv*wouldLieIfAsked_8m_nv)\n",
    " \n",
    "    PSVL_F_NI_10d10m_nv = (1-r_nv)*PSVL_NF_NI_10d10m_nv + r_nv*np.mean(hStar_F_10d10m_nv*doesSvyIfAsked_NI_10d10m_nv*wouldLieIfAsked_nv)\n",
    "    PSVL_F_I_10d10m_nv = (1-r_nv)*PSVL_NF_I_10d10m_nv + r_nv*np.mean(hStar_F_10d10m_nv*doesSvyIfAsked_I_10d10m_nv*wouldLieIfAsked_nv)\n",
    "    PSVL_F_NI_10d10m_8m_nv = (1-r_nv)*PSVL_NF_NI_10d10m_8m_nv + r_nv*np.mean(hStar_F_10d10m_nv*doesSvyIfAsked_NI_10d10m_nv*wouldLieIfAsked_8m_nv)\n",
    "    PSVL_F_I_10d10m_8m_nv = (1-r_nv)*PSVL_NF_I_10d10m_8m_nv + r_nv*np.mean(hStar_F_10d10m_nv*doesSvyIfAsked_I_10d10m_nv*wouldLieIfAsked_8m_nv)\n",
    " \n",
    "    PSVL_FV_NI_10d10m_nv = (1-r_nv)*PSVL_NF_NI_10d10m_nv + r_nv*np.mean(hStar_FV_10d10m_nv*doesSvyIfAsked_I_10d10m_nv*wouldLieIfAsked_nv)\n",
    "    PSVL_FV_I_10d10m_nv = (1-r_nv)*PSVL_NF_I_10d10m_nv + r_nv*np.mean(hStar_FV_10d10m_nv*doesSvyIfAsked_I_10d10m_nv*wouldLieIfAsked_nv)\n",
    "    PSVL_FV_NI_10d10m_8m_nv = (1-r_nv)*PSVL_NF_NI_10d10m_8m_nv + r_nv*np.mean(hStar_FV_10d10m_nv*doesSvyIfAsked_I_10d10m_nv*wouldLieIfAsked_8m_nv)\n",
    "    PSVL_FV_I_10d10m_8m_nv = (1-r_nv)*PSVL_NF_I_10d10m_8m_nv + r_nv*np.mean(hStar_FV_10d10m_nv*doesSvyIfAsked_I_10d10m_nv*wouldLieIfAsked_8m_nv)\n",
    " \n",
    "    PSVL_OO_NI_10d10m_nv = (1-r_nv)*PSVL_NF_NI_10d10m_nv + r_nv*np.mean((1-optsOutIfSees_OO_10d10m_nv)*hStar_F_10d10m_nv*doesSvyIfAsked_NI_10d10m_nv*wouldLieIfAsked_nv)\n",
    "    PSVL_OO_I_10d10m_nv = (1-r_nv)*PSVL_NF_I_10d10m_nv + r_nv*np.mean((1-optsOutIfSees_OO_10d10m_nv)*hStar_F_10d10m_nv*doesSvyIfAsked_I_10d10m_nv*wouldLieIfAsked_nv)\n",
    "    PSVL_OO_NI_10d10m_8m_nv = (1-r_nv)*PSVL_NF_NI_10d10m_8m_nv + r_nv*np.mean((1-optsOutIfSees_OO_10d10m_nv)*hStar_F_10d10m_nv*doesSvyIfAsked_NI_10d10m_nv*wouldLieIfAsked_8m_nv)\n",
    "    PSVL_OO_I_10d10m_8m_nv = (1-r_nv)*PSVL_NF_I_10d10m_8m_nv + r_nv*np.mean((1-optsOutIfSees_OO_10d10m_nv)*hStar_F_10d10m_nv*doesSvyIfAsked_I_10d10m_nv*wouldLieIfAsked_8m_nv)\n",
    "    \n",
    "    PSVL_OOV_NI_10d10m_nv = (1-r_nv)*PSVL_NF_NI_10d10m_nv + r_nv*np.mean((1-optsOutIfSees_OOV_10d10m_nv)*hStar_FV_10d10m_nv*doesSvyIfAsked_I_10d10m_nv*wouldLieIfAsked_nv)\n",
    "    PSVL_OOV_I_10d10m_nv = (1-r_nv)*PSVL_NF_I_10d10m_nv + r_nv*np.mean((1-optsOutIfSees_OOV_10d10m_nv)*hStar_FV_10d10m_nv*doesSvyIfAsked_I_10d10m_nv*wouldLieIfAsked_nv)\n",
    "    PSVL_OOV_NI_10d10m_8m_nv = (1-r_nv)*PSVL_NF_NI_10d10m_8m_nv + r_nv*np.mean((1-optsOutIfSees_OOV_10d10m_nv)*hStar_FV_10d10m_nv*doesSvyIfAsked_I_10d10m_nv*wouldLieIfAsked_8m_nv)\n",
    "    PSVL_OOV_I_10d10m_8m_nv = (1-r_nv)*PSVL_NF_I_10d10m_8m_nv + r_nv*np.mean((1-optsOutIfSees_OOV_10d10m_nv)*hStar_FV_10d10m_nv*doesSvyIfAsked_I_10d10m_nv*wouldLieIfAsked_8m_nv)\n",
    "     \n",
    "# 10d5m, 5d1m incentive\n",
    "    PSVL_NF_NI_10d5m_nv = np.mean(h0_nv*doesSvyIfAsked_NI_10d5m_nv*wouldLieIfAsked_nv)\n",
    "    PSVL_NF_I_10d5m_nv = np.mean(h0_nv*doesSvyIfAsked_I_10d5m_nv*wouldLieIfAsked_nv)\n",
    "    PSVL_NF_NI_10d5m_5d1m_nv = np.mean(h0_nv*doesSvyIfAsked_NI_10d5m_nv*wouldLieIfAsked_5d1m_nv)\n",
    "    PSVL_NF_I_10d5m_5d1m_nv = np.mean(h0_nv*doesSvyIfAsked_I_10d5m_nv*wouldLieIfAsked_5d1m_nv)\n",
    " \n",
    "    PSVL_F_NI_10d5m_nv = (1-r_nv)*PSVL_NF_NI_10d5m_nv + r_nv*np.mean(hStar_F_10d5m_nv*doesSvyIfAsked_NI_10d5m_nv*wouldLieIfAsked_nv)\n",
    "    PSVL_F_I_10d5m_nv = (1-r_nv)*PSVL_NF_I_10d5m_nv + r_nv*np.mean(hStar_F_10d5m_nv*doesSvyIfAsked_I_10d5m_nv*wouldLieIfAsked_nv)\n",
    "    PSVL_F_NI_10d5m_5d1m_nv = (1-r_nv)*PSVL_NF_NI_10d5m_5d1m_nv + r_nv*np.mean(hStar_F_10d5m_nv*doesSvyIfAsked_NI_10d5m_nv*wouldLieIfAsked_5d1m_nv)\n",
    "    PSVL_F_I_10d5m_5d1m_nv = (1-r_nv)*PSVL_NF_I_10d5m_5d1m_nv + r_nv*np.mean(hStar_F_10d5m_nv*doesSvyIfAsked_I_10d5m_nv*wouldLieIfAsked_5d1m_nv)\n",
    " \n",
    "    PSVL_FV_NI_10d5m_nv = (1-r_nv)*PSVL_NF_NI_10d5m_nv + r_nv*np.mean(hStar_FV_10d5m_nv*doesSvyIfAsked_I_10d5m_nv*wouldLieIfAsked_nv)\n",
    "    PSVL_FV_I_10d5m_nv = (1-r_nv)*PSVL_NF_I_10d5m_nv + r_nv*np.mean(hStar_FV_10d5m_nv*doesSvyIfAsked_I_10d5m_nv*wouldLieIfAsked_nv)\n",
    "    PSVL_FV_NI_10d5m_5d1m_nv = (1-r_nv)*PSVL_NF_NI_10d5m_5d1m_nv + r_nv*np.mean(hStar_FV_10d5m_nv*doesSvyIfAsked_I_10d5m_nv*wouldLieIfAsked_5d1m_nv)\n",
    "    PSVL_FV_I_10d5m_5d1m_nv = (1-r_nv)*PSVL_NF_I_10d5m_5d1m_nv + r_nv*np.mean(hStar_FV_10d5m_nv*doesSvyIfAsked_I_10d5m_nv*wouldLieIfAsked_5d1m_nv)\n",
    " \n",
    "    PSVL_OO_NI_10d5m_nv = (1-r_nv)*PSVL_NF_NI_10d5m_nv + r_nv*np.mean((1-optsOutIfSees_OO_10d5m_nv)*hStar_F_10d5m_nv*doesSvyIfAsked_NI_10d5m_nv*wouldLieIfAsked_nv)\n",
    "    PSVL_OO_I_10d5m_nv = (1-r_nv)*PSVL_NF_I_10d5m_nv + r_nv*np.mean((1-optsOutIfSees_OO_10d5m_nv)*hStar_F_10d5m_nv*doesSvyIfAsked_I_10d5m_nv*wouldLieIfAsked_nv)\n",
    "    PSVL_OO_NI_10d5m_5d1m_nv = (1-r_nv)*PSVL_NF_NI_10d5m_5d1m_nv + r_nv*np.mean((1-optsOutIfSees_OO_10d5m_nv)*hStar_F_10d5m_nv*doesSvyIfAsked_NI_10d5m_nv*wouldLieIfAsked_5d1m_nv)\n",
    "    PSVL_OO_I_10d5m_5d1m_nv = (1-r_nv)*PSVL_NF_I_10d5m_5d1m_nv + r_nv*np.mean((1-optsOutIfSees_OO_10d5m_nv)*hStar_F_10d5m_nv*doesSvyIfAsked_I_10d5m_nv*wouldLieIfAsked_5d1m_nv)\n",
    " \n",
    "    PSVL_OOV_NI_10d5m_nv = (1-r_nv)*PSVL_NF_NI_10d5m_nv + r_nv*np.mean((1-optsOutIfSees_OOV_10d5m_nv)*hStar_FV_10d5m_nv*doesSvyIfAsked_I_10d5m_nv*wouldLieIfAsked_nv)\n",
    "    PSVL_OOV_I_10d5m_nv = (1-r_nv)*PSVL_NF_I_10d5m_nv + r_nv*np.mean((1-optsOutIfSees_OOV_10d5m_nv)*hStar_FV_10d5m_nv*doesSvyIfAsked_I_10d5m_nv*wouldLieIfAsked_nv)\n",
    "    PSVL_OOV_NI_10d5m_5d1m_nv = (1-r_nv)*PSVL_NF_NI_10d5m_5d1m_nv + r_nv*np.mean((1-optsOutIfSees_OOV_10d5m_nv)*hStar_FV_10d5m_nv*doesSvyIfAsked_I_10d5m_nv*wouldLieIfAsked_5d1m_nv)\n",
    "    PSVL_OOV_I_10d5m_5d1m_nv = (1-r_nv)*PSVL_NF_I_10d5m_5d1m_nv + r_nv*np.mean((1-optsOutIfSees_OOV_10d5m_nv)*hStar_FV_10d5m_nv*doesSvyIfAsked_I_10d5m_nv*wouldLieIfAsked_5d1m_nv)\n",
    "\n",
    "# !!! 30 PH moments (5 tx * 3 length * 2 v/nv) !!!\n",
    "    sm = np.zeros(101)\n",
    "    sm[0] = PH_NF_0d5m_v\n",
    "    sm[1] = PH_NF_10d10m_v\n",
    "    sm[2] = PH_NF_10d5m_v\n",
    "    sm[3] = PH_F_0d5m_v\n",
    "    sm[4] = PH_F_10d10m_v\n",
    "    sm[5] = PH_F_10d5m_v\n",
    "    sm[6] = PH_FV_0d5m_v\n",
    "    sm[7] = PH_FV_10d10m_v\n",
    "    sm[8] = PH_FV_10d5m_v\n",
    "    sm[9] = PH_OO_0d5m_v\n",
    "    sm[10] = PH_OO_10d10m_v\n",
    "    sm[11] = PH_OO_10d5m_v\n",
    "    sm[12] = PH_OOV_0d5m_v\n",
    "    sm[13] = PH_OOV_10d10m_v\n",
    "    sm[14] = PH_OOV_10d5m_v\n",
    "    sm[15] = PH_NF_0d5m_nv\n",
    "    sm[16] = PH_NF_10d10m_nv\n",
    "    sm[17] = PH_NF_10d5m_nv\n",
    "    sm[18] = PH_F_0d5m_nv\n",
    "    sm[19] = PH_F_10d10m_nv\n",
    "    sm[20] = PH_F_10d5m_nv\n",
    "    sm[21] = PH_FV_0d5m_nv\n",
    "    sm[22] = PH_FV_10d10m_nv\n",
    "    sm[23] = PH_FV_10d5m_nv\n",
    "    sm[24] = PH_OO_0d5m_nv\n",
    "    sm[25] = PH_OO_10d10m_nv\n",
    "    sm[26] = PH_OO_10d5m_nv\n",
    "    sm[27] = PH_OOV_0d5m_nv\n",
    "    sm[28] = PH_OOV_10d10m_nv\n",
    "    sm[29] = PH_OOV_10d5m_nv\n",
    "\n",
    "# 30 PSV moments (5 tx * 3 length * 2 v/nv). Taking 50/50 average across NI and I treatments\n",
    "    sm[30]  =   np.mean([  PSV_NF_I_0d5m_v,     PSV_NF_NI_0d5m_v    ])\n",
    "    sm[31]  =   np.mean([  PSV_NF_I_10d10m_v,   PSV_NF_NI_10d10m_v  ])\n",
    "    sm[32]  =   np.mean([  PSV_NF_I_10d5m_v,    PSV_NF_NI_10d5m_v   ]) \n",
    "    sm[33]  =   np.mean([  PSV_F_I_0d5m_v,      PSV_F_NI_0d5m_v     ]) \n",
    "    sm[34]  =   np.mean([  PSV_F_I_10d10m_v,    PSV_F_NI_10d10m_v   ]) \n",
    "    sm[35]  =   np.mean([  PSV_F_I_10d5m_v,     PSV_F_NI_10d5m_v    ]) \n",
    "    sm[36]  =   np.mean([  PSV_FV_I_0d5m_v,     PSV_FV_NI_0d5m_v    ]) \n",
    "    sm[37]  =   np.mean([  PSV_FV_I_10d10m_v,   PSV_FV_NI_10d10m_v  ]) \n",
    "    sm[38]  =   np.mean([  PSV_FV_I_10d5m_v,    PSV_FV_NI_10d5m_v   ]) \n",
    "    sm[39]  =   np.mean([  PSV_OO_I_0d5m_v,     PSV_OO_NI_0d5m_v    ]) \n",
    "    sm[40] =   np.mean([  PSV_OO_I_10d10m_v,   PSV_OO_NI_10d10m_v  ]) \n",
    "    sm[41] =   np.mean([  PSV_OO_I_10d5m_v,    PSV_OO_NI_10d5m_v   ]) \n",
    "    sm[42] =   np.mean([  PSV_OOV_I_0d5m_v,    PSV_OOV_NI_0d5m_v   ]) \n",
    "    sm[43] =   np.mean([  PSV_OOV_I_10d10m_v,  PSV_OOV_NI_10d10m_v ]) \n",
    "    sm[44] =   np.mean([  PSV_OOV_I_10d5m_v,   PSV_OOV_NI_10d5m_v  ]) \n",
    "    sm[45] =   np.mean([  PSV_NF_I_0d5m_nv,    PSV_NF_NI_0d5m_nv   ]) \n",
    "    sm[46] =   np.mean([  PSV_NF_I_10d10m_nv,  PSV_NF_NI_10d10m_nv ]) \n",
    "    sm[47] =   np.mean([  PSV_NF_I_10d5m_nv,   PSV_NF_NI_10d5m_nv  ]) \n",
    "    sm[48] =   np.mean([  PSV_F_I_0d5m_nv,     PSV_F_NI_0d5m_nv    ]) \n",
    "    sm[49] =   np.mean([  PSV_F_I_10d10m_nv,   PSV_F_NI_10d10m_nv  ]) \n",
    "    sm[50] =   np.mean([  PSV_F_I_10d5m_nv,    PSV_F_NI_10d5m_nv   ]) \n",
    "    sm[51] =   np.mean([  PSV_FV_I_0d5m_nv,    PSV_FV_NI_0d5m_nv   ]) \n",
    "    sm[52] =   np.mean([  PSV_FV_I_10d10m_nv,  PSV_FV_NI_10d10m_nv ]) \n",
    "    sm[53] =   np.mean([  PSV_FV_I_10d5m_nv,   PSV_FV_NI_10d5m_nv  ]) \n",
    "    sm[54] =   np.mean([  PSV_OO_I_0d5m_nv,    PSV_OO_NI_0d5m_nv   ]) \n",
    "    sm[55] =   np.mean([  PSV_OO_I_10d10m_nv,  PSV_OO_NI_10d10m_nv ]) \n",
    "    sm[56] =   np.mean([  PSV_OO_I_10d5m_nv,   PSV_OO_NI_10d5m_nv  ]) \n",
    "    sm[57] =   np.mean([  PSV_OOV_I_0d5m_nv,   PSV_OOV_NI_0d5m_nv  ]) \n",
    "    sm[58] =   np.mean([  PSV_OOV_I_10d10m_nv, PSV_OOV_NI_10d10m_nv]) \n",
    "    sm[59] =   np.mean([  PSV_OOV_I_10d5m_nv,  PSV_OOV_NI_10d5m_nv ])\n",
    "\n",
    "# 12 POO moments (2 tx * 3 length * 2 v/nv)\n",
    "    sm[60] = POO_OO_0d5m_v\n",
    "    sm[61] = POO_OO_10d10m_v\n",
    "    sm[62] = POO_OO_10d5m_v\n",
    "    sm[63] = POO_OOV_0d5m_v\n",
    "    sm[64] = POO_OOV_10d10m_v\n",
    "    sm[65] = POO_OOV_10d5m_v\n",
    "    sm[66] = POO_OO_0d5m_nv\n",
    "    sm[67] = POO_OO_10d10m_nv\n",
    "    sm[68] = POO_OO_10d5m_nv\n",
    "    sm[69] = POO_OOV_0d5m_nv\n",
    "    sm[70] = POO_OOV_10d10m_nv\n",
    "    sm[71] = POO_OOV_10d5m_nv\n",
    "\n",
    "# 20 PSV by info moments (5 tx * 2 I/NI * 2 v/nv)\n",
    "    sm[72]   =   np.mean([  PSV_NF_NI_0d5m_v,    PSV_NF_NI_10d10m_v,      PSV_NF_NI_10d5m_v   ])\n",
    "    sm[73]   =   np.mean([  PSV_NF_I_0d5m_v,     PSV_NF_I_10d10m_v,       PSV_NF_I_10d5m_v  ])\n",
    "    sm[74]   =   np.mean([  PSV_F_NI_0d5m_v,     PSV_F_NI_10d10m_v,       PSV_F_NI_10d5m_v    ])\n",
    "    sm[75]   =   np.mean([  PSV_F_I_0d5m_v,      PSV_F_I_10d10m_v,       PSV_F_I_10d5m_v   ])\n",
    "    sm[76]   =   np.mean([  PSV_FV_NI_0d5m_v,    PSV_FV_NI_10d10m_v,      PSV_FV_NI_10d5m_v   ])\n",
    "    sm[77]   =   np.mean([  PSV_FV_I_0d5m_v,     PSV_FV_I_10d10m_v,       PSV_FV_I_10d5m_v  ])\n",
    "    sm[78]   =   np.mean([  PSV_OO_NI_0d5m_v,    PSV_OO_NI_10d10m_v,      PSV_OO_NI_10d5m_v   ])\n",
    "    sm[79]   =   np.mean([  PSV_OO_I_0d5m_v,     PSV_OO_I_10d10m_v,       PSV_OO_I_10d5m_v  ])\n",
    "    sm[80]   =   np.mean([  PSV_OOV_NI_0d5m_v,   PSV_OOV_NI_10d10m_v,     PSV_OOV_NI_10d5m_v  ])\n",
    "    sm[81]   =   np.mean([  PSV_OOV_I_0d5m_v,    PSV_OOV_I_10d10m_v,      PSV_OOV_I_10d5m_v ])\n",
    "    sm[82]   =   np.mean([  PSV_NF_NI_0d5m_nv,   PSV_NF_NI_10d10m_nv,     PSV_NF_NI_10d5m_nv  ])\n",
    "    sm[83]   =   np.mean([  PSV_NF_I_0d5m_nv,    PSV_NF_I_10d10m_nv,      PSV_NF_I_10d5m_nv ])\n",
    "    sm[84]   =   np.mean([  PSV_F_NI_0d5m_nv,    PSV_F_NI_10d10m_nv,      PSV_F_NI_10d5m_nv   ])\n",
    "    sm[85]   =   np.mean([  PSV_F_I_0d5m_nv,     PSV_F_I_10d10m_nv,       PSV_F_I_10d5m_nv  ])\n",
    "    sm[86]   =   np.mean([  PSV_FV_NI_0d5m_nv,   PSV_FV_NI_10d10m_nv,     PSV_FV_NI_10d5m_nv  ])\n",
    "    sm[87]   =   np.mean([  PSV_FV_I_0d5m_nv,    PSV_FV_I_10d10m_nv,      PSV_FV_I_10d5m_nv ])\n",
    "    sm[88]   =   np.mean([  PSV_OO_NI_0d5m_nv,   PSV_OO_NI_10d10m_nv,     PSV_OO_NI_10d5m_nv  ])\n",
    "    sm[89]   =   np.mean([  PSV_OO_I_0d5m_nv,    PSV_OO_I_10d10m_nv,      PSV_OO_I_10d5m_nv ])\n",
    "    sm[90]   =   np.mean([  PSV_OOV_NI_0d5m_nv,  PSV_OOV_NI_10d10m_nv,    PSV_OOV_NI_10d5m_nv ])\n",
    "    sm[91]   =   np.mean([  PSV_OOV_I_0d5m_nv,   PSV_OOV_I_10d10m_nv,     PSV_OOV_I_10d5m_nv ])\n",
    "\n",
    "# 8 PL moments (1 tx * 2 10m/5m * 2 incentives)\n",
    "\n",
    "# Empirical moments are sum of people lying in relevant tx divided by the sum of people answering the survey in relevant tx.\n",
    "    sm[92] = np.mean([PSVL_NF_NI_0d5m_v, PSVL_NF_I_0d5m_v, PSVL_NF_NI_10d5m_v, PSVL_NF_I_10d5m_v,\n",
    "                    PSVL_F_NI_0d5m_v, PSVL_F_I_0d5m_v, PSVL_F_NI_10d5m_v, PSVL_F_I_10d5m_v,\n",
    "                    PSVL_FV_NI_0d5m_v, PSVL_FV_I_0d5m_v, PSVL_FV_NI_10d5m_v, PSVL_FV_I_10d5m_v,\n",
    "                    PSVL_OO_NI_0d5m_v, PSVL_OO_I_0d5m_v, PSVL_OO_NI_10d5m_v, PSVL_OO_I_10d5m_v,\n",
    "                    PSVL_OOV_NI_0d5m_v, PSVL_OOV_I_0d5m_v, PSVL_OOV_NI_10d5m_v, PSVL_OOV_I_10d5m_v])/np.mean([PSV_NF_NI_0d5m_v,\n",
    "                    PSV_NF_I_0d5m_v, PSV_NF_NI_10d5m_v, PSV_NF_I_10d5m_v,\n",
    "                    PSV_F_NI_0d5m_v, PSV_F_I_0d5m_v, PSV_F_NI_10d5m_v, PSV_F_I_10d5m_v,\n",
    "                    PSV_FV_NI_0d5m_v, PSV_FV_I_0d5m_v, PSV_FV_NI_10d5m_v, PSV_FV_I_10d5m_v,\n",
    "                    PSV_OO_NI_0d5m_v, PSV_OO_I_0d5m_v,  PSV_OO_NI_10d5m_v, PSV_OO_I_10d5m_v,\n",
    "                    PSV_OOV_NI_0d5m_v, PSV_OOV_I_0d5m_v, PSV_OOV_NI_10d5m_v, PSV_OOV_I_10d5m_v])\n",
    "\n",
    "    sm[93] = np.mean([PSVL_NF_NI_0d5m_5d1m_v, PSVL_NF_I_0d5m_5d1m_v, PSVL_NF_NI_10d5m_5d1m_v, PSVL_NF_I_10d5m_5d1m_v,\n",
    "                    PSVL_F_NI_0d5m_5d1m_v, PSVL_F_I_0d5m_5d1m_v, PSVL_F_NI_10d5m_5d1m_v, PSVL_F_I_10d5m_5d1m_v,\n",
    "                    PSVL_FV_NI_0d5m_5d1m_v, PSVL_FV_I_0d5m_5d1m_v, PSVL_FV_NI_10d5m_5d1m_v, PSVL_FV_I_10d5m_5d1m_v,\n",
    "                    PSVL_OO_NI_0d5m_5d1m_v, PSVL_OO_I_0d5m_5d1m_v, PSVL_OO_NI_10d5m_5d1m_v, PSVL_OO_I_10d5m_5d1m_v,\n",
    "                    PSVL_OOV_NI_0d5m_5d1m_v, PSVL_OOV_I_0d5m_5d1m_v, PSVL_OOV_NI_10d5m_5d1m_v, PSVL_OOV_I_10d5m_5d1m_v])/ np.mean([PSV_NF_NI_0d5m_v,\n",
    "                    PSV_NF_I_0d5m_v, PSV_NF_NI_10d5m_v, PSV_NF_I_10d5m_v,\n",
    "                    PSV_F_NI_0d5m_v, PSV_F_I_0d5m_v, PSV_F_NI_10d5m_v, PSV_F_I_10d5m_v,\n",
    "                    PSV_FV_NI_0d5m_v, PSV_FV_I_0d5m_v, PSV_FV_NI_10d5m_v, PSV_FV_I_10d5m_v,\n",
    "                    PSV_OO_NI_0d5m_v, PSV_OO_I_0d5m_v, PSV_OO_NI_10d5m_v, PSV_OO_I_10d5m_v,\n",
    "                    PSV_OOV_NI_0d5m_v, PSV_OOV_I_0d5m_v, PSV_OOV_NI_10d5m_v, PSV_OOV_I_10d5m_v])   \n",
    "\n",
    "    sm[94] = np.mean([PSVL_NF_NI_10d10m_v, PSVL_NF_I_10d10m_v,\n",
    "                    PSVL_F_NI_10d10m_v, PSVL_F_I_10d10m_v,\n",
    "                    PSVL_FV_NI_10d10m_v, PSVL_FV_I_10d10m_v,\n",
    "                    PSVL_OO_NI_10d10m_v, PSVL_OO_I_10d10m_v,\n",
    "                    PSVL_OOV_NI_10d10m_v, PSVL_OOV_I_10d10m_v])/ np.mean([PSV_NF_NI_10d10m_v,\n",
    "                    PSV_NF_I_10d10m_v,\n",
    "                    PSV_F_NI_10d10m_v, PSV_F_I_10d10m_v,\n",
    "                    PSV_FV_NI_10d10m_v, PSV_FV_I_10d10m_v,\n",
    "                    PSV_OO_NI_10d10m_v, PSV_OO_I_10d10m_v,\n",
    "                    PSV_OOV_NI_10d10m_v, PSV_OOV_I_10d10m_v])\n",
    "                \n",
    "    sm[95] = np.mean([PSVL_NF_NI_10d10m_8m_v, PSVL_NF_I_10d10m_8m_v,\n",
    "                    PSVL_F_NI_10d10m_8m_v, PSVL_F_I_10d10m_8m_v,\n",
    "                    PSVL_FV_NI_10d10m_8m_v, PSVL_FV_I_10d10m_8m_v,\n",
    "                    PSVL_OO_NI_10d10m_8m_v, PSVL_OO_I_10d10m_8m_v,\n",
    "                    PSVL_OOV_NI_10d10m_8m_v, PSVL_OOV_I_10d10m_8m_v])/ np.mean([PSV_NF_NI_10d10m_v,\n",
    "                    PSV_NF_I_10d10m_v,\n",
    "                    PSV_F_NI_10d10m_v, PSV_F_I_10d10m_v, PSV_FV_NI_10d10m_v, PSV_FV_I_10d10m_v,\n",
    "                    PSV_OO_NI_10d10m_v, PSV_OO_I_10d10m_v,\n",
    "                    PSV_OOV_NI_10d10m_v, PSV_OOV_I_10d10m_v]) \n",
    "                \n",
    "    sm[96] = np.mean([PSVL_NF_NI_0d5m_nv, PSVL_NF_I_0d5m_nv, PSVL_NF_NI_10d5m_nv, PSVL_NF_I_10d5m_nv,\n",
    "                    PSVL_F_NI_0d5m_nv, PSVL_F_I_0d5m_nv, PSVL_F_NI_10d5m_nv, PSVL_F_I_10d5m_nv,\n",
    "                    PSVL_FV_NI_0d5m_nv, PSVL_FV_I_0d5m_nv, PSVL_FV_NI_10d5m_nv, PSVL_FV_I_10d5m_nv, \n",
    "                    PSVL_OO_NI_0d5m_nv, PSVL_OO_I_0d5m_nv, PSVL_OO_NI_10d5m_nv, PSVL_OO_I_10d5m_nv,\n",
    "                    PSVL_OOV_NI_0d5m_nv, PSVL_OOV_I_0d5m_nv, PSVL_OOV_NI_10d5m_nv, PSVL_OOV_I_10d5m_nv])/ np.mean([PSV_NF_NI_0d5m_nv,\n",
    "                    PSV_NF_I_0d5m_nv, PSV_NF_NI_10d5m_nv, PSV_NF_I_10d5m_nv,\n",
    "                    PSV_F_NI_0d5m_nv, PSV_F_I_0d5m_nv, PSV_F_NI_10d5m_nv, PSV_F_I_10d5m_nv,\n",
    "                    PSV_FV_NI_0d5m_nv, PSV_FV_I_0d5m_nv, PSV_FV_NI_10d5m_nv, PSV_FV_I_10d5m_nv,\n",
    "                    PSV_OO_NI_0d5m_nv, PSV_OO_I_0d5m_nv,  PSV_OO_NI_10d5m_nv, PSV_OO_I_10d5m_nv,\n",
    "                    PSV_OOV_NI_0d5m_nv, PSV_OOV_I_0d5m_nv, PSV_OOV_NI_10d5m_nv, PSV_OOV_I_10d5m_nv]) \n",
    " \n",
    "    sm[97] = np.mean([PSVL_NF_NI_0d5m_5d1m_nv, PSVL_NF_I_0d5m_5d1m_nv, PSVL_NF_NI_10d5m_5d1m_nv, PSVL_NF_I_10d5m_5d1m_nv,\n",
    "                    PSVL_F_NI_0d5m_5d1m_nv, PSVL_F_I_0d5m_5d1m_nv, PSVL_F_NI_10d5m_5d1m_nv, PSVL_F_I_10d5m_5d1m_nv,\n",
    "                    PSVL_FV_NI_0d5m_5d1m_nv, PSVL_FV_I_0d5m_5d1m_nv,PSVL_FV_NI_10d5m_5d1m_nv, PSVL_FV_I_10d5m_5d1m_nv,\n",
    "                    PSVL_OO_NI_0d5m_5d1m_nv, PSVL_OO_I_0d5m_5d1m_nv, PSVL_OO_NI_10d5m_5d1m_nv, PSVL_OO_I_10d5m_5d1m_nv,\n",
    "                    PSVL_OOV_NI_0d5m_5d1m_nv, PSVL_OOV_I_0d5m_5d1m_nv, PSVL_OOV_NI_10d5m_5d1m_nv, PSVL_OOV_I_10d5m_5d1m_nv ])/ np.mean([PSV_NF_NI_0d5m_nv, \n",
    "                    PSV_NF_I_0d5m_nv, PSV_NF_NI_10d5m_nv, PSV_NF_I_10d5m_nv,\n",
    "                    PSV_F_NI_0d5m_nv, PSV_F_I_0d5m_nv, PSV_F_NI_10d5m_nv, PSV_F_I_10d5m_nv,\n",
    "                    PSV_FV_NI_0d5m_nv, PSV_FV_I_0d5m_nv, PSV_FV_NI_10d5m_nv, PSV_FV_I_10d5m_nv,\n",
    "                    PSV_OO_NI_0d5m_nv, PSV_OO_I_0d5m_nv,  PSV_OO_NI_10d5m_nv, PSV_OO_I_10d5m_nv,\n",
    "                    PSV_OOV_NI_0d5m_nv, PSV_OOV_I_0d5m_nv, PSV_OOV_NI_10d5m_nv, PSV_OOV_I_10d5m_nv])\n",
    "\n",
    "    sm[98] = np.mean([PSVL_NF_NI_10d10m_nv, PSVL_NF_I_10d10m_nv,\n",
    "                    PSVL_F_NI_10d10m_nv, PSVL_F_I_10d10m_nv,\n",
    "                    PSVL_FV_NI_10d10m_nv, PSVL_FV_I_10d10m_nv,\n",
    "                    PSVL_OO_NI_10d10m_nv, PSVL_OO_I_10d10m_nv,\n",
    "                    PSVL_OOV_NI_10d10m_nv, PSVL_OOV_I_10d10m_nv])/ np.mean([PSV_NF_NI_10d10m_nv,\n",
    "                    PSV_NF_I_10d10m_nv,\n",
    "                    PSV_F_NI_10d10m_nv, PSV_F_I_10d10m_nv,\n",
    "                    PSV_FV_NI_10d10m_nv, PSV_FV_I_10d10m_nv,\n",
    "                    PSV_OO_NI_10d10m_nv, PSV_OO_I_10d10m_nv,\n",
    "                    PSV_OOV_NI_10d10m_nv, PSV_OOV_I_10d10m_nv])\n",
    "                \n",
    "    sm[99] = np.mean([PSVL_NF_NI_10d10m_8m_nv, PSVL_NF_I_10d10m_8m_nv,\n",
    "                    PSVL_F_NI_10d10m_8m_nv, PSVL_F_I_10d10m_8m_nv,\n",
    "                    PSVL_FV_NI_10d10m_8m_nv, PSVL_FV_I_10d10m_8m_nv,\n",
    "                    PSVL_OO_NI_10d10m_8m_nv, PSVL_OO_I_10d10m_8m_nv,\n",
    "                    PSVL_OOV_NI_10d10m_8m_nv, PSVL_OOV_I_10d10m_8m_nv])/ np.mean([PSV_NF_NI_10d10m_nv,\n",
    "                    PSV_NF_I_10d10m_nv,\n",
    "                    PSV_F_NI_10d10m_nv, PSV_F_I_10d10m_nv,\n",
    "                    PSV_FV_NI_10d10m_nv, PSV_FV_I_10d10m_nv,\n",
    "                    PSV_OO_NI_10d10m_nv, PSV_OO_I_10d10m_nv,\n",
    "                    PSV_OOV_NI_10d10m_nv, PSV_OOV_I_10d10m_nv])\n",
    "    \n",
    "    sm[100] =   Turnout_control # % of simulated individuals who vote\n",
    "    return sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2f829a",
   "metadata": {},
   "source": [
    "## 3. Estimation\n",
    "\n",
    "### Point estimates and standard errors\n",
    "\n",
    "We now estimate the model using simulated mininum distance. We use as minimization algorithm \"Nelder-Mead\" that behaves similarly to Matlab's fminsearch. We do not impose bounds like the authors did since they do not appear to matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf31a98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number N of simulated individuals and the draws from a standard normal distribution to compute the simulated s_v, s_nv, Ïµ, s \n",
    "\n",
    "sim_voters = 750000\n",
    "rand0 = np.random.normal(0,1,sim_voters)\n",
    "rand1 = np.random.normal(0,1,sim_voters)\n",
    "rand2 = np.random.normal(0,1,sim_voters)\n",
    "rand3 = np.random.normal(0,1,sim_voters)\n",
    "rand_vec = np.array([rand0,rand1,rand2,rand3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ed8ef7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the function we want to minimize: the weighted sum of the squared differences between empirical and simulated moments\n",
    "\n",
    "def criterion(parameters, rand_set):\n",
    "    \n",
    "    simMoments = voteSimEndogenousVoting_vary(parameters,rand_set)\n",
    "    m = np.subtract(emp_moments, simMoments)\n",
    "    y = m.T @ W @ m\n",
    "    \n",
    "    return y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf4fdf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the quasi-random starting guesses for the parameters. These are found in part B of the appendix\n",
    "\n",
    "h0_v_in = np.random.uniform(low=0.2,high=0.4,size=1)\n",
    "h0_nv_in = np.random.uniform(low=0.2,high=0.4,size=1)\n",
    "r_v_in = np.random.uniform(low=0.2,high=0.4,size=1)\n",
    "r_nv_in = np.random.uniform(low=0.2,high=0.4,size=1)\n",
    "eta_v_in = np.random.uniform(low=0.0,high=0.5,size=1)\n",
    "eta_nv_in = np.random.uniform(low=0.0,high=0.5,size=1)\n",
    "mu_s_v_in = np.random.uniform(low=-50.0,high=0.0,size=1)\n",
    "mu_s_nv_in = np.random.uniform(low=-50.0,high=0.0,size=1)\n",
    "sigma_s_v_in = np.random.uniform(low=0.0,high=50.0,size=1)\n",
    "sigma_s_nv_in = np.random.uniform(low=0.0,high=50.0,size=1)\n",
    "S_svy_v_in = np.random.uniform(low=0.0,high=10.0,size=1)\n",
    "S_svy_nv_in = np.random.uniform(low=0.0,high=10.0,size=1)\n",
    "timeval_v_in = np.random.uniform(low=0.0,high=100.0,size=1)\n",
    "timeval_nv_in = np.random.uniform(low=0.0,high=100.0,size=1)\n",
    "mu_sv_in = np.random.uniform(low=-20.0,high=20.0,size=1)\n",
    "mu_sn_in = np.random.uniform(low=-30.0,high=10.0,size=1)\n",
    "sigma_svn_in = np.random.uniform(low=0.0,high=30.0,size=1)\n",
    "L_in = np.random.uniform(low=0.0,high=20.0,size=1)\n",
    "mu_eps_in = np.random.uniform(low=-30.0,high=100.0,size=1)\n",
    "sigma_eps_in = np.random.uniform(low=50.0,high=200.0,size=1)\n",
    "\n",
    "params_init=[h0_v_in,h0_nv_in,r_v_in,r_nv_in,eta_v_in,eta_nv_in,mu_s_v_in,mu_s_nv_in,sigma_s_v_in,\n",
    "             sigma_s_nv_in,S_svy_v_in,S_svy_nv_in,timeval_v_in,timeval_nv_in,mu_sv_in,mu_sn_in,\n",
    "             sigma_svn_in,L_in,mu_eps_in,sigma_eps_in]\n",
    "params_init = [item for sublist in params_init for item in sublist] # flatten the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5fcc9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n"
     ]
    }
   ],
   "source": [
    "# !!! Read before running !!!\n",
    "\n",
    "# This cell computes the estimates. The number of iterations needed for convergence depends a lot on the starting guesses, \n",
    "# nonetheless 5000 iterations are usually sufficient to get a wsse (weighted sum of squared errors) of around 160 which is the one\n",
    "# found also by the authors. \n",
    "\n",
    "# The minimization procedure is computationally intensive and it takes a lot of time to reach convergence. On our machines it\n",
    "# takes around a couple of hours for 500 iterations, so we suggest to run our Julia code if you want to get the estimates faster (500 \n",
    "# iterations in Julia take around 10-13 minutes). \n",
    "\n",
    "# We will minimize in the next cells our criterion function starting from a point \"relatively close\" to our best estimates to \n",
    "# show that the algorithm works and converge to the same solutions found in julia.\n",
    "\n",
    "# !!! We changed the max number of iterations to 0 in this cell so you don't accidentally start a really long routine. If you want to find the estimates substitute 0 with 5000 !!!\n",
    "\n",
    "sol = opt.minimize(criterion,params_init,rand_vec,method='Nelder-Mead',options={'disp': True, 'adaptive':True, 'maxiter': 0})\n",
    "results = sol.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37d309be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the estimation procedure can take a lot of time and depends on the rand_vec and the starting guesses, here we provide \n",
    "# the data to replicate our best estimates. The file estimates2 contains the estimates found by the authors, our best estimates,\n",
    "# another set of estimates we found that performs well and the initial parameters we used to found them. random_vector_used \n",
    "# contains the rand_vec we used to compute our best estimates\n",
    "\n",
    "best_estimates = pd.read_csv('../input/estimates2.csv')             # our best estimates\n",
    "best_random_vec = pd.read_csv('../input/random_vector_used.csv')\n",
    "rand_vec_used = [best_random_vec[\"rand1\"],best_random_vec[\"rand2\"],best_random_vec[\"rand3\"],best_random_vec[\"rand4\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cacd19f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n"
     ]
    }
   ],
   "source": [
    "# !!! Read before running !!!\n",
    "\n",
    "# In this cell we show that we converge to the same solutions found in Julia (what we called best_estimates). We use as starting\n",
    "# guesses values close to the best estimates we found to speed up converge and reduce the number of iterations needed.\n",
    "\n",
    "# This will still take a couple of hours to converge. Please change maxiter from 0 to 800 to get the estimates  \n",
    "\n",
    "params_init_close = best_estimates[\"myestimates\"] + 0.3\n",
    "sol = opt.minimize(criterion,params_init_close,rand_vec_used,method='Nelder-Mead',\n",
    "                   options={'disp': True, 'adaptive':True, 'maxiter': 0})\n",
    "results = sol.x;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fee56594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The difference in the weighted sum of squared errors is: 0.1125\n"
     ]
    }
   ],
   "source": [
    "# Compare our best estimates with the ones obtained by the authors\n",
    "\n",
    "# These are the authors' estimates\n",
    "parameters_authors = [0.38,0.36,0.38,0.30,0.14,0.16,-22.6,-27.7,26.9,24.7,1.6,1.2,42.7,23.9,-3.9,-11.3,9.5,7.6,64.1,318.7]\n",
    "\n",
    "# The difference is positive if our estimates have a lower weighted sum of squared errors. Remember that here we are using our random vector and not the authors' seed\n",
    "dif = criterion(parameters_authors,rand_vec) - criterion(best_estimates[\"myestimates\"],rand_vec)\n",
    "\n",
    "print(\"The difference in the weighted sum of squared errors is: \" + str(round(dif,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15ce176",
   "metadata": {},
   "source": [
    "We now compute standard errors. The description that follows is taken from the paper's appendix. \n",
    "\n",
    "The simulated method of moments estimator that uses W as weigthing matrix achieves asymptotic normality, with estimated variance:\n",
    "\n",
    "$$ (\\hat{G}'W\\hat{G})^{-1}(\\hat{G}'W(1+J_m/J_s)\\hat{\\Lambda}W\\hat{G}) (\\hat{G}'W\\hat{G})^{-1}/N $$\n",
    "\n",
    "Where J<sub>m</sub> is the number of empirical observations used to compute a moment and J<sub>s</sub> is the number of simulated observations used to compute a simulated moment. The other terms are:\n",
    "\n",
    "$$ \\hat{G} \\equiv N^{-1}\\sum_{i=1}^N \\nabla_\\xi m_i(\\hat{\\xi})  \\qquad \\qquad \\hat{\\Lambda} \\equiv Var(m(\\hat{\\xi})$$\n",
    "\n",
    "In the code we will use instead of G the jacobian of the criterion function computed numerically using finite differences, and instead of &Lambda; the var-cov matrix of the empirical moments. W is the same weighting matrix used before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dda3f4e",
   "metadata": {},
   "source": [
    "We will now compute the Jacobian numerically. We will NOT use this jacobian to compute the standard errors since we get two columns of zeroes for the gradient with respect to mu_eps and sigma_eps and so the matrix is not invertible. This problem is either due to how python computes the jacobian or to mistakes in the function we wrote to compute the simulated moments. By using the same parameters and the same random vectors we see in fact very small differences (max difference of 0.002, many other way smaller or equal to zero. note that all moments are probabilities) between our and the authors' simulated moments. We checked but did not spot any mistake in the function we wrote. The dissimilarities between simulated moments could be because of different \"default options\" between python and matlab's operators (eg rounding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75996c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the jacobian using finite differences\n",
    "# To use our jacobian we would need to take only the first 18 parameters, so using jac[1:18,1:18] instead of jac_matlab. By doing this\n",
    "# we loose though the estimates for the standard errors of mu_eps and sigma_eps\n",
    "\n",
    "# !!! This cell will take a while to run !!!\n",
    "\n",
    "fjac = ndt.Jacobian(voteSimEndogenousVoting_vary)\n",
    "jac = fjac(best_estimates[\"myestimates\"],rand_vec_used)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f59474",
   "metadata": {},
   "source": [
    "We load the jacobian evaluated in our best estimates computed in Matlab using the \"Jacobianest\" function (the same used by the authors). Another possible problem with the jacobian computed above using finite differences is that (citing a comment in the \"Jacobianest\" function): \"The error term on these (finite differences jacobian) estimates has a second order component, but also some 4th and 6th order terms in it\". The Matlab code in \"Jacobianest\" then uses Romberg extrapolation to improve the estimate while this improvement is not available in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e1d7132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load matlab's jacobian\n",
    "\n",
    "jac_matlab = pd.read_csv('../input/jac_mat.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c5ef6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the standard errors. We use the same notation as their Matlab code\n",
    "# sim_adjust is a scalar = 1 + J_m / J_s\n",
    "# DFDY_CSD_jacest is our jacobian (101x20 matrix) evaluated in the minimum\n",
    "# W is the weighting matrix (101x101 diagonal matrix), \n",
    "# VCcontrol is the variance-covariance matrix of the empirical moments (a 101x101 matrix).\n",
    "\n",
    "Jm_Js = 13197 / sim_voters\n",
    "sim_adjust = 1 + Jm_Js\n",
    "DFDY_CSD_jacest = jac_matlab\n",
    "\n",
    "A = DFDY_CSD_jacest.T @ W @ DFDY_CSD_jacest\n",
    "B = DFDY_CSD_jacest.T @ W @ (sim_adjust*emp_moments_varcov) @ W @ DFDY_CSD_jacest\n",
    "\n",
    "# Var-cov matrix for the estimates is by computing A\\B/A\n",
    "\n",
    "VC1 = np.linalg.lstsq(A,B,rcond=None)[0]           # Matrix left division A\\B\n",
    "VC  = np.linalg.lstsq(A.T, VC1.T,rcond=None)[0].T  # Matrix right division (A\\B)/A\n",
    "\n",
    "standard_errors = np.sqrt(np.diag(VC))             # standard errors are the square root of the diagonal elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784e0111",
   "metadata": {},
   "source": [
    "## 4. Print and Save Estimation Results\n",
    "\n",
    "We print a table with our estimates and the authors' to facilitate the comparison and save it as a csv file.\n",
    "\n",
    "##### About the differences with the paper's results:\n",
    "\n",
    "We see some differences in the point estimates we obtained compared to the ones in the paper. This is partially to be expected given the highly complex nature of the minimization problem and the randomness in computing the simulated moments. The authors in the paper run the minimization procedure 720 times changing the starting condition and then pick the estimates that give the lowest weighted sum of squared errors. Because of time constraints we only run one minimization on python and around a dozen in julia and report the estimates that give us the lowest sse. There are some parameters that are easily identified and do not change across different minimization procedures, for example the baseline probability of being at home or the probability of seeing the flyer. Other parameters are estimated differently in different minimization routines but their value do not change too much, like the social value of voting for voters and non-voters or the lying cost. The only parameters that vary a lot across different estimation procedures are mu_eps and sigma_eps. For a more detailed discussion on the difficulties to estimate these two parameters please refer to section 5.6 in the paper where the authors talk about this issue and the relevant robustness checks they implement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9cb162e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TABLE 3: Simulated minimum-distance estimates, benchmark results with heterogeneous auxiliary parameters\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parameters</th>\n",
       "      <th>our_point_est</th>\n",
       "      <th>our_se</th>\n",
       "      <th>authors_point_est</th>\n",
       "      <th>authors_se</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>h0_v</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.0089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>h0_nv</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.0092</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.0092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>r_v</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.0203</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.0204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>r_nv</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.0183</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.0183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eta_v</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.1545</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.1232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>eta_nv</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.2652</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.2987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>mu_s_v</td>\n",
       "      <td>-22.75</td>\n",
       "      <td>2.8442</td>\n",
       "      <td>-22.60</td>\n",
       "      <td>2.9580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mu_s_nv</td>\n",
       "      <td>-31.10</td>\n",
       "      <td>4.9122</td>\n",
       "      <td>-27.70</td>\n",
       "      <td>5.5938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sigma_s_v</td>\n",
       "      <td>26.50</td>\n",
       "      <td>5.3195</td>\n",
       "      <td>26.90</td>\n",
       "      <td>5.5176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sigma_s_nv</td>\n",
       "      <td>28.46</td>\n",
       "      <td>5.6799</td>\n",
       "      <td>24.70</td>\n",
       "      <td>6.6687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>S_svy_v</td>\n",
       "      <td>1.33</td>\n",
       "      <td>1.1427</td>\n",
       "      <td>1.60</td>\n",
       "      <td>1.2084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>S_svy_nv</td>\n",
       "      <td>1.09</td>\n",
       "      <td>1.5946</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.6149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>timeval_v</td>\n",
       "      <td>46.55</td>\n",
       "      <td>10.3671</td>\n",
       "      <td>42.70</td>\n",
       "      <td>9.4438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>timeval_nv</td>\n",
       "      <td>20.32</td>\n",
       "      <td>13.9636</td>\n",
       "      <td>23.90</td>\n",
       "      <td>15.1539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mu_sv</td>\n",
       "      <td>-4.30</td>\n",
       "      <td>1.4205</td>\n",
       "      <td>-3.90</td>\n",
       "      <td>1.4858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>mu_sn</td>\n",
       "      <td>-11.92</td>\n",
       "      <td>2.1310</td>\n",
       "      <td>-11.30</td>\n",
       "      <td>1.6422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>sigma_si</td>\n",
       "      <td>9.88</td>\n",
       "      <td>2.0011</td>\n",
       "      <td>9.50</td>\n",
       "      <td>2.9750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>L_in</td>\n",
       "      <td>6.80</td>\n",
       "      <td>1.2318</td>\n",
       "      <td>7.60</td>\n",
       "      <td>1.5247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>mu_eps</td>\n",
       "      <td>20.86</td>\n",
       "      <td>15.0609</td>\n",
       "      <td>64.10</td>\n",
       "      <td>61.9195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>sigma_eps</td>\n",
       "      <td>137.93</td>\n",
       "      <td>66.1336</td>\n",
       "      <td>318.70</td>\n",
       "      <td>248.6525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    parameters  our_point_est   our_se  authors_point_est  authors_se\n",
       "0         h0_v           0.38   0.0089               0.38      0.0089\n",
       "1        h0_nv           0.36   0.0092               0.36      0.0092\n",
       "2          r_v           0.38   0.0203               0.38      0.0204\n",
       "3         r_nv           0.30   0.0183               0.30      0.0183\n",
       "4        eta_v           0.17   0.1545               0.14      0.1232\n",
       "5       eta_nv           0.18   0.2652               0.16      0.2987\n",
       "6       mu_s_v         -22.75   2.8442             -22.60      2.9580\n",
       "7      mu_s_nv         -31.10   4.9122             -27.70      5.5938\n",
       "8    sigma_s_v          26.50   5.3195              26.90      5.5176\n",
       "9   sigma_s_nv          28.46   5.6799              24.70      6.6687\n",
       "10     S_svy_v           1.33   1.1427               1.60      1.2084\n",
       "11    S_svy_nv           1.09   1.5946               1.20      1.6149\n",
       "12   timeval_v          46.55  10.3671              42.70      9.4438\n",
       "13  timeval_nv          20.32  13.9636              23.90     15.1539\n",
       "14       mu_sv          -4.30   1.4205              -3.90      1.4858\n",
       "15       mu_sn         -11.92   2.1310             -11.30      1.6422\n",
       "16    sigma_si           9.88   2.0011               9.50      2.9750\n",
       "17        L_in           6.80   1.2318               7.60      1.5247\n",
       "18      mu_eps          20.86  15.0609              64.10     61.9195\n",
       "19   sigma_eps         137.93  66.1336             318.70    248.6525"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a new dataframe with the results and save it as a csv file\n",
    "\n",
    "col = ['h0_v','h0_nv','r_v','r_nv','eta_v','eta_nv','mu_s_v','mu_s_nv','sigma_s_v','sigma_s_nv','S_svy_v','S_svy_nv',\n",
    "       'timeval_v','timeval_nv','mu_sv','mu_sn','sigma_si','L_in','mu_eps','sigma_eps']\n",
    "se_authors= [0.0089,0.0092,0.0204,0.0183,0.1232,0.2987,2.9580,5.5938,5.5176,6.6687,1.2084,1.6149,9.4438,15.1539,1.4858,1.6422,\n",
    "             2.9750,1.5247,61.9195,248.6525]\n",
    "table3 = pd.DataFrame({'parameters':col,'our_point_est':np.around(best_estimates[\"myestimates\"],2),'our_se':np.around(standard_errors,4),\n",
    "                       'authors_point_est':parameters_authors,'authors_se':se_authors})\n",
    "table3.to_csv('../output/estimates.csv')\n",
    "\n",
    "print(\"TABLE 3: Simulated minimum-distance estimates, benchmark results with heterogeneous auxiliary parameters\")\n",
    "display(table3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
